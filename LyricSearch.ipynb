{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlrmluRfNayp"
   },
   "source": [
    "First, we install the lyricsgenius API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2v6oNAIxM1TN",
    "outputId": "0912025a-a0ea-47e6-8445-0e0c3cb6360c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: multiprocess in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from multiprocess) (0.3.4)\n",
      "Requirement already satisfied: lyricsgenius in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from lyricsgenius) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from lyricsgenius) (2.26.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from beautifulsoup4>=4.6.0->lyricsgenius) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (3.3)\n",
      "Requirement already satisfied: metapy in c:\\users\\joshu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install multiprocess\n",
    "\n",
    "!pip install lyricsgenius\n",
    "!pip install metapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the libraries and set a path for our input file of artists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AwPBO5kVbh-h"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import multiprocess\n",
    "import queue\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from requests.exceptions import HTTPError, ConnectionError, RequestException\n",
    "from lyricsgenius import Genius\n",
    "\n",
    "\n",
    "# OS agnostic\n",
    "import os \n",
    "CSV_PATH = os.path.join(os.path.curdir, 'artists', '10000-MTV-Music-Artists-page-%s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjAsUT5ZOSex"
   },
   "source": [
    "# Scrape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a lyricsgenius token, and use the API to pull the lyrics data for each artist in the dataset for the top 10,000 artists from MTV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zBQzXBJNabo"
   },
   "outputs": [],
   "source": [
    "# Genius setup\n",
    "             \n",
    "def genius_setup():\n",
    "    token = \"EBufquOcw_ts4Y4V7yiddUNyUakTdqCpnMZhiI3XtAScWOntEom8Hj4T87gAV_cA\"\n",
    "    genius = Genius(token, retries=2)\n",
    "\n",
    "    genius.verbose = False\n",
    "    genius.remove_section_headers = True\n",
    "    genius.skip_non_songs = True\n",
    "    genius.excluded_terms = [\"(Remix)\", \"(Live)\"]\n",
    "\n",
    "    return genius    \n",
    "# Multiprocessing cores\n",
    "process_number = int(multiprocess.cpu_count()) * 2\n",
    "\n",
    "# Data management\n",
    "final_ = multiprocess.Manager().list()\n",
    "\n",
    "# artist_queue = queue.Queue()\n",
    "# final_ = []\n",
    "checked_artists = set()\n",
    "\n",
    "file_name = \"song_data_2.csv\"\n",
    "\n",
    "# Pull out artists\n",
    "def get_artists(queue):\n",
    "    for x in range(1,5):\n",
    "        path = CSV_PATH % str(x)\n",
    "        with open(path, encoding=\"UTF-8\") as csvfile:\n",
    "            TopArtists = csv.reader(csvfile)\n",
    "            \n",
    "            # Skip header\n",
    "            next(TopArtists)\n",
    "            for row in TopArtists:\n",
    "                artist = row[0]\n",
    "                # Check if we should skip this artists since we already found the data\n",
    "                if artist not in checked_artists:\n",
    "                    queue.put(artist)\n",
    "                      \n",
    "\n",
    "\n",
    "# File management\n",
    "def write_to_csv(data):\n",
    "    \"\"\"\n",
    "    data: list of dictionaries {artist, song, data}\n",
    "    \"\"\"\n",
    "    global file_name\n",
    "    \n",
    "    csv_path = os.path.join(os.path.curdir, 'data', file_name)\n",
    "    with open(csv_path, 'w') as csv_file: \n",
    "        # creating a csv dict writer object \n",
    "        print(\"Entries: {num}\".format(num=len(data)))\n",
    "        keys = data[0].keys()\n",
    "        writer = csv.DictWriter(csv_file, fieldnames = keys) \n",
    "        \n",
    "        # writing headers (field names) \n",
    "        writer.writeheader() \n",
    "        \n",
    "        # writing data rows \n",
    "        writer.writerows(data) \n",
    "        \n",
    "\n",
    "def read_csv():\n",
    "    global final_, checked_artists, file_name   \n",
    "    \n",
    "    csv_path = os.path.join(os.path.curdir, 'data', file_name)\n",
    "    \n",
    "    # opening the CSV file\n",
    "    try:\n",
    "        with open(csv_path, mode ='r', encoding=\"UTF-8\") as file:   \n",
    "\n",
    "            # reading the CSV file\n",
    "            data = csv.DictReader(file)\n",
    "\n",
    "            for entry in data:\n",
    "                checked_artists.add(entry[\"artist\"])\n",
    "                final_.append(entry)\n",
    "                \n",
    "        print(\"Number of artists already found {num}\".format(num=len(checked_artists)))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "# Run genius search\n",
    "def search_genius(args):\n",
    "    import sys\n",
    "    from requests.exceptions import RequestException\n",
    "    artist_queue, num, genius, final_ = args\n",
    "    \n",
    "    def log(string):\n",
    "        print(\"[{num}] \".format(num=num) + string + \"\\n\", end='')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    # Processing\n",
    "    def clean_data(data):\n",
    "        cleaned_data = data.replace(\"\\n\", \"|\").replace(\",\", \" \")\n",
    "        return cleaned_data\n",
    "\n",
    "    def process_artist(artist):\n",
    "        artist_dict = artist.to_dict()\n",
    "        return \"\"\n",
    "\n",
    "    def process_song(song):\n",
    "        lyrics = clean_data(song.lyrics)\n",
    "        return lyrics\n",
    "\n",
    "    def build_entry(artist, song, data, columns = [\"artist\", \"song\", \"data\"]):\n",
    "        entry = {\"artist\": artist, \"song\": song, \"data\": data}\n",
    "        return entry\n",
    "    \n",
    "    log(\"Starting\")\n",
    "    try:\n",
    "        while True:\n",
    "            genius_artist = None\n",
    "            artist = artist_queue.get()\n",
    "            if artist is None:\n",
    "                log(\"Done\")\n",
    "                return\n",
    "            log(\"Remaining: [{queue}]. Searching {artist}\".format(queue=artist_queue.qsize(), artist=artist.strip()))\n",
    "            \n",
    "            # Pull data for artist from genius\n",
    "            for x in range(5):\n",
    "                try:\n",
    "                    genius_artist = genius.search_artist(artist, per_page=50, get_full_info=False)\n",
    "                    break\n",
    "                except RequestException as e:\n",
    "                    log(\"HTTPSConnectionPool exception. Attempt {}/5\".format(x+1))\n",
    "                except Exception as e:\n",
    "                    log(\"Exception. Attempt {}/3\".format(x+1))\n",
    "            \n",
    "            log(\"Finished {artist}\".format(num=num, artist=artist.strip()))\n",
    "            if genius_artist == None:\n",
    "                log(\"{artist} not found\".format(num=num, artist=artist.strip()))\n",
    "                continue\n",
    "                           \n",
    "            artist_data =  process_artist(genius_artist)\n",
    "                           \n",
    "            log(\"{artist} number of songs: {song_num}\".format(num=num, artist=artist.strip(), song_num=len(genius_artist.songs)))\n",
    "            \n",
    "            for song in genius_artist.songs:\n",
    "                song_data = process_song(song)\n",
    "                \n",
    "                # Add to final list\n",
    "                final_.append(build_entry(artist, song.title, song_data))\n",
    "    \n",
    "    except Exception as e:\n",
    "        log(\"Something went wrong: {error}\".format(num=num, error= e))\n",
    "    \n",
    "    \n",
    "def run(multi_core=False): \n",
    "    \n",
    "    # Setup Genius\n",
    "    genius = genius_setup()\n",
    "    \n",
    "    # Load in any previous data\n",
    "    print(\"Reading previous\")\n",
    "    read_csv()\n",
    "    \n",
    "    pool = None\n",
    "    try:  \n",
    "        if multi_core:\n",
    "            # multiprocess.log_to_stderr().setLevel(logging.DEBUG)\n",
    "            print(\"Multiprocessing with {process_number} processes\".format(process_number=process_number))\n",
    "            \n",
    "            artist_queue = multiprocess.Manager().Queue()\n",
    "            get_artists(artist_queue)\n",
    "            \n",
    "            for x in range(process_number):\n",
    "                artist_queue.put(None)\n",
    "            \n",
    "            print(artist_queue.qsize())\n",
    "            # creating processes\n",
    "            with multiprocess.get_context(\"spawn\").Pool(process_number) as pool:\n",
    "                args = [(artist_queue, x, genius, final_) for x in range(process_number)]\n",
    "                pool.map(search_genius, args)\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "            \n",
    "        else:\n",
    "            print(\"Running single core\")\n",
    "            artist_queue = queue.Queue()\n",
    "            get_artists(artist_queue)\n",
    "            artist_queue.put(None)\n",
    "            print(artist_queue.qsize())\n",
    "            search_genius((artist_queue, 0, genius, final_))\n",
    "\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        if pool:\n",
    "            pool.close()\n",
    "            pool.terminate()\n",
    "            pool.join()\n",
    "        print(\"KeyboardInterrupt: Writing results\")\n",
    "    \n",
    "    finally:\n",
    "        write_to_csv(list(final_))                       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "run(multi_core=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius = genius_setup()\n",
    "genius_artist = genius.search_artist(\"Sam Hunt\", per_page=50, get_full_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_artist.songs[30].lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing \n",
    "Now we tokenize the lyrics into stemmed, lowercase unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metapy\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# OS agnostic\n",
    "import os \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\"\n",
    "    data: a string to tokenize\n",
    "    \n",
    "    tokens: a list of tokenized ngrams\n",
    "    \"\"\"\n",
    "    doc = metapy.index.Document()\n",
    "    doc.content(data)\n",
    "\n",
    "    tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "    tok = metapy.analyzers.LowercaseFilter(tok)\n",
    "    tok = metapy.analyzers.Porter2Filter(tok)     \n",
    "    ana = metapy.analyzers.NGramWordAnalyzer(1, tok)\n",
    "    trigrams = ana.analyze(doc)\n",
    "    tok.set_content(doc.content())\n",
    "\n",
    "    tokens, counts = [], []\n",
    "    for token, count in trigrams.items():\n",
    "        tokens.append(token)\n",
    "        counts.append(count)    \n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_file(input_file=\"song_data.csv\", output_file=\"song_data_tokenize.csv\"):\n",
    "    \"\"\"\n",
    "    processes a file (artist, song, data) into tokenized lyrics\n",
    "    currently only processes first song (?)\n",
    "    \n",
    "    file_name: file to tokenize data rows in\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing data in\", input_file)\n",
    "\n",
    "    # set our path and temp file to write to - safer\n",
    "    input_csv_path = os.path.join(os.path.curdir, 'data', input_file)\n",
    "    output_csv_path = os.path.join(os.path.curdir, 'data', output_file)\n",
    "    tempfile = NamedTemporaryFile('w+t', newline='', delete=False)\n",
    "    \n",
    "    # read in each lyric and tokenize it as a metapy document\n",
    "    try:\n",
    "        with open(input_csv_path, mode ='r+', encoding = 'utf-8') as file, tempfile:   \n",
    "            # read from main file, write to temp file\n",
    "            reader = csv.DictReader(file)\n",
    "            writer = csv.DictWriter(tempfile, extrasaction='ignore', \n",
    "                                    fieldnames=['artist', 'song', 'data'])\n",
    "            \n",
    "            for row in reader:\n",
    "                # print(\"Tokenizing song\", row['song'])\n",
    "                try:\n",
    "                    row['data'] = tokenize(row['data'])\n",
    "                    writer.writerow(row)\n",
    "                except UnicodeDecodeError:\n",
    "                    print(\"Error decoding sonng {}\".format(row['song']))\n",
    "            \n",
    "            shutil.move(tempfile.name, output_csv_path)\n",
    "    \n",
    "    except (FileNotFoundError) as err:\n",
    "        print(err)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec \n",
    "Using doc2vec to turn out sets of lyrics in to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sklearn\n",
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file=\"song_data_2.csv\"\n",
    "input_csv_path = os.path.join(os.path.curdir, 'data', input_file)\n",
    "\n",
    "def get_data():\n",
    "    documents = []\n",
    "    with open(input_csv_path, mode ='r+', encoding = 'utf-8') as file:   \n",
    "            datareader = csv.DictReader(file)\n",
    "            \n",
    "            next(datareader)\n",
    "            for row in datareader:\n",
    "                yield row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(data):\n",
    "    tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "    tok = metapy.analyzers.LowercaseFilter(tok)\n",
    "    tok.set_content(data)\n",
    "    tokens = []\n",
    "    for t in tok:\n",
    "        if not \"embed\" in t:\n",
    "            tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_documents = []\n",
    "size = len(documents)\n",
    "documents = get_data()\n",
    "for x in range(1000):\n",
    "    \n",
    "    document = next(documents)\n",
    "    \n",
    "    data = document['data']\n",
    "    lines = data.split(\"|\")\n",
    "    \n",
    "    song = document['song']\n",
    "    artist = document['artist']\n",
    "    \n",
    "    for line in lines:\n",
    "        tokens = tokenize_words(line)\n",
    "        tokenized_documents.append(TaggedDocument(tokens, [(song, artist)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "model = Doc2Vec(tokenized_documents, dm=1, vector_size=100, negative=5, hs=0, sample = 0, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "I'm tired of gettin' drunk, tired of bein' free\n",
    "\"\"\"\n",
    "test_doc = tokenize_words(doc.lower())\n",
    "model.dv.most_similar(positive=[model.infer_vector(test_doc)],topn=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install tensorflow\n",
    "!pip3 install tensorflow_hub\n",
    "!pip3 install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joshu\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import tensorflow_hub as hub\n",
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input file and a generator function to fetch the file row by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"song_data_2.csv\"\n",
    "input_csv_path = os.path.join(os.path.curdir, 'data', input_file)\n",
    "\n",
    "def get_rows():\n",
    "    \"\"\"\n",
    "    Returns a generator for each row of the CSV file\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(input_csv_path, mode ='r+', encoding = 'utf-8') as file:   \n",
    "            datareader = csv.DictReader(file)\n",
    "            \n",
    "            next(datareader)\n",
    "            for row in datareader:\n",
    "                yield row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define several helper functions to load documents in batches, fetch TF embeddings, and initialize our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_load():\n",
    "    \"\"\"\n",
    "    Load a batch of 1000 songs\n",
    "    \"\"\"\n",
    "    print(\"loading batch\")\n",
    "    \n",
    "    batch_sentences = []\n",
    "    global counter\n",
    "    \n",
    "    for x in range(1000):\n",
    "        \n",
    "        document = next(documents)\n",
    "\n",
    "        data = document['data']\n",
    "        lines = data.split(\"|\")\n",
    "\n",
    "        song = document['song']\n",
    "        artist = document['artist']\n",
    "        for line in lines:\n",
    "            ids[counter] = (line, song, artist)\n",
    "            batch_sentences.append(line)\n",
    "            counter += 1\n",
    "        \n",
    "    return batch_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    Gets embeddings for a given line (document)\n",
    "    \"\"\"\n",
    "    print(\"getting embeddings\")\n",
    "    \n",
    "    embed_module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")\n",
    "    placeholder = tf.placeholder(dtype=tf.string)\n",
    "    embed = embed_module(placeholder)\n",
    "    session = tf.Session()\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    \n",
    "    embeddings = session.run(embed, feed_dict={placeholder: sentences})\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mapping and counter to keep track of documents\n",
    "counter = 0\n",
    "mapping = {}\n",
    "documents = get_rows()\n",
    "\n",
    "# Initialize the ANNOY index \n",
    "ann = AnnoyIndex(512, metric='angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding items to index\n",
      "getting embeddings\n",
      "adding items to index\n",
      "getting embeddings\n",
      "building index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_lines(batch_num=1000):\n",
    "    \"\"\"\n",
    "    Split up a document into lines\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for x in range(batch_num):\n",
    "        row = next(documents)\n",
    "        \n",
    "        if row == None:\n",
    "            break\n",
    "            \n",
    "        data = row['data']\n",
    "        lyrics = data.split(\"|\")\n",
    "        song = row['song']\n",
    "        artist = row['artist']\n",
    "        \n",
    "        for lyric in lyrics:\n",
    "            lines.append([lyric, song, artist])\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "    \n",
    "def add_items_to_index(sentences):\n",
    "    \"\"\"\n",
    "    Builds an ANNOY index\n",
    "    \"\"\"\n",
    "    print(\"adding items to index\")\n",
    "    \n",
    "    global ann, counter, mapping\n",
    "    \n",
    "    sent = [x[0] for x in sentences]\n",
    "    \n",
    "    embeddings = get_embeddings(sent)\n",
    "    for x, embed in enumerate(embeddings):\n",
    "        ann.add_item(counter, embed)\n",
    "        mapping[counter] = sentences[x] # maps only the song\n",
    "        counter +=1\n",
    "    \n",
    "\n",
    "        \n",
    "# while True: # run entire data set\n",
    "for x in range(2): # run just a couple batches\n",
    "    lines = split_lines()\n",
    "    if len(lines) == 0:\n",
    "        break\n",
    "    add_items_to_index(lines)\n",
    "\n",
    "print(\"building index\")\n",
    "ann.build(n_trees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting embeddings\n",
      "getting nearest neighbors\n",
      "Closest: \n",
      "Old Shit -  Miranda Lambert :\n",
      "     Redman tobacco  Grandpa's two cents     \n",
      "     Old timers  \"there in a pinch\"     \n",
      "     I'm a fan of it  old shit     \n",
      "     Splittin logs  smokin hogs     \n",
      "==== Feedin leftovers to a three legged dog ====\n",
      "     I'm a fan of it  old shit     \n",
      "          \n",
      "     One man's trash is another man's treasure     \n",
      "Little Red Wagon -  Miranda Lambert :\n",
      "     I live in Oklahoma     \n",
      "     And I've got long  blonde hair     \n",
      "     And I play guitar and I go on the road     \n",
      "     And I do all the shit you wanna do     \n",
      "==== And my dog does tricks ====\n",
      "     And I ain't about drama y'all     \n",
      "     I love my apron     \n",
      "     But I ain't your momma     \n",
      "It’s Christmas Time -  Joey + Rory :\n",
      "     It's Christmas time  who's at the door     \n",
      "     I'm sure there's room for 7 more     \n",
      "     We'll make some pallets on the floor     \n",
      "     That'll be just fine     \n",
      "==== The turkeys done  pull up a chair ====\n",
      "     Grab a hand  let's say a prayer     \n",
      "     For all those soldiers over there     \n",
      "     You know it's Christmas time     \n",
      "Dirt Road Diary -  Luke Bryan :\n",
      "     Yeah  yeah     \n",
      "     Oh  oh     \n",
      "     Blood brothers4EmbedShare URLCopyEmbedCopy     \n",
      "     Me and daddy'd ride around all day shooting doves off a line in a Chevrolet     \n",
      "==== Old lab would jump out the back and fetch them up ====\n",
      "     We would drive for miles and miles and never once hit blacktop or change the dial     \n",
      "     One little country station was all there was     \n",
      "     Checking gates  fixing fence and roads     \n",
      "Pet Cheetah -  Twenty One Pilots :\n",
      "          \n",
      "     I've got a pet cheetah down in my basement     \n",
      "     I've raised him  and bathed him  and named him Jason     \n",
      "     Statham  I've trained him to make me these beats     \n",
      "==== Now my pet cheetah's quicker in the studio than on his feet ====\n",
      "     I'ma get mine and get going     \n",
      "     I'm showing my faces in just enough places     \n",
      "     I'm done with tip-toeing  I'll stay in my room     \n",
      "Little Boys Grow Up and Dogs Get Old -  Luke Bryan :\n",
      "     With her little jeans and white tee     \n",
      "     She might rev up an old boy's heart or even stop one     \n",
      "     Putting on a show tonight  letting go tonight     \n",
      "     She might be a mess  but she's a hot oneEmbedShare URLCopyEmbedCopy     \n",
      "==== Bandit was a black lab  Daddy brought home ====\n",
      "     He said  \"Son he's yours from now on\"     \n",
      "     Bandit looked at me and he cocked his head     \n",
      "     Slept at night on the foot of my bed     \n",
      "Big Green Tractor -  Jason Aldean :\n",
      "     I'm hittin' easy street on mud tires     \n",
      "     That's right34EmbedShare URLCopyEmbedCopy     \n",
      "     She had a shiny little Beamer with the ragtop down     \n",
      "     Sittin' in the drive but she wouldn't get out     \n",
      "==== The dogs were all barkin' and a-waggin' around ====\n",
      "     And I just laughed and said \"Y'all get in\"     \n",
      "          \n",
      "     She had on a new dress and she'd curled her hair     \n",
      "Wild Weekend -  Luke Bryan :\n",
      "     Sunburnt lips on me  yeah  her sunburnt lipsEmbedShare URLCopyEmbedCopy     \n",
      "     We've been getting beat up by this old life     \n",
      "     Seems all we got left is Friday and Saturday night and sometimes Sunday if Monday is a holiday     \n",
      "     I got a bunch of buddies I aren’t seen in a while     \n",
      "==== I got some good-time fun and hot dog buns in a stock pile ====\n",
      "     With some home brew     \n",
      "     Malibu and Miller Lite     \n",
      "     Girl put on that short little party dress     \n",
      "What Child Is This? -  Carrie Underwood :\n",
      "     Yeah3EmbedShare URLCopyEmbedCopy     \n",
      "     What child is this who lay to rest     \n",
      "     On Mary's lap is sleeping     \n",
      "     Whom angels greet with anthem sweet     \n",
      "==== While shepherds watch our keeping? ====\n",
      "     This  this is Christ  the King     \n",
      "     Whom shepherds guard and angels sing     \n",
      "     Haste  haste to bring Him  Lord     \n",
      "The More Boys I Meet -  Carrie Underwood :\n",
      "          \n",
      "     And I  I close my eyes     \n",
      "     And  I kiss that frog     \n",
      "     Each time finding     \n",
      "==== The more boys I meet  the more I love my dog ====\n",
      "          \n",
      "     Here's this guy  thinks he's bad to the bone     \n",
      "     He wants to pick me up and take me home     \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input_sentence = \"I'm sick of sittin' at the house, dyin' on my phone\"\n",
    "input_sentence = \"feeding the dog\"\n",
    "\n",
    "query_embeddings = get_embeddings([input_sentence])[0]\n",
    "\n",
    "#Return 10 nearest neighbors\n",
    "print(\"getting nearest neighbors\")\n",
    "nns = ann.get_nns_by_vector(query_embeddings, 10, include_distances=False)\n",
    "\n",
    "print(\"Closest: \")\n",
    "for item in nns:\n",
    "    print(\"{} - {}:\".format(mapping[item][1], mapping[item][2]))\n",
    "    for x in range(item-4, item+4):\n",
    "        if x == item:\n",
    "            print(\"==== {} ====\".format(mapping[x][0]))\n",
    "        else:\n",
    "            print(\"     {}     \".format(mapping[x][0]))\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS410 Project Bae Area",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
