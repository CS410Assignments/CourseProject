{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlrmluRfNayp"
   },
   "source": [
    "First, we install the lyricsgenius API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2v6oNAIxM1TN",
    "outputId": "0912025a-a0ea-47e6-8445-0e0c3cb6360c"
   },
   "outputs": [],
   "source": [
    "!pip install multiprocess\n",
    "\n",
    "!pip install lyricsgenius\n",
    "!pip install metapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the libraries and set a path for our input file of artists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwPBO5kVbh-h"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import multiprocess\n",
    "import queue\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from lyricsgenius import Genius\n",
    "import metapy\n",
    "\n",
    "\n",
    "# OS agnostic\n",
    "import os \n",
    "CSV_PATH = os.path.join(os.path.curdir, 'artists', '10000-MTV-Music-Artists-page-%s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjAsUT5ZOSex"
   },
   "source": [
    "# Scrape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a lyricsgenius token, and use the API to pull the lyrics data for each artist in the dataset for the top 10,000 artists from MTV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zBQzXBJNabo"
   },
   "outputs": [],
   "source": [
    "# Genius setup\n",
    "             \n",
    "def genius_setup():\n",
    "    token = \"EBufquOcw_ts4Y4V7yiddUNyUakTdqCpnMZhiI3XtAScWOntEom8Hj4T87gAV_cA\"\n",
    "    genius = Genius(token, retries=2)\n",
    "\n",
    "    genius.verbose = False\n",
    "    genius.remove_section_headers = True\n",
    "    genius.skip_non_songs = True\n",
    "    genius.excluded_terms = [\"(Remix)\", \"(Live)\"]\n",
    "\n",
    "    return genius    \n",
    "# Multiprocessing cores\n",
    "process_number = int(multiprocess.cpu_count()) * 2\n",
    "\n",
    "# Data management\n",
    "final_ = multiprocess.Manager().list()\n",
    "\n",
    "# artist_queue = queue.Queue()\n",
    "# final_ = []\n",
    "checked_artists = set()\n",
    "\n",
    "# Pull out artists\n",
    "def get_artists(queue):\n",
    "    for x in range(1,5):\n",
    "        path = CSV_PATH % str(x)\n",
    "        with open(path, encoding=\"UTF-8\") as csvfile:\n",
    "            TopArtists = csv.reader(csvfile)\n",
    "            \n",
    "            # Skip header\n",
    "            next(TopArtists)\n",
    "            for row in TopArtists:\n",
    "                artist = row[0]\n",
    "                # Check if we should skip this artists since we already found the data\n",
    "                if artist not in checked_artists:\n",
    "                    queue.put(artist)\n",
    "                      \n",
    "\n",
    "\n",
    "# File management\n",
    "def write_to_csv(data, file_name=\"song_data.csv\"):\n",
    "    \"\"\"\n",
    "    data: list of dictionaries {artist, song, data}\n",
    "    \"\"\"\n",
    "    \n",
    "    csv_path = os.path.join(os.path.curdir, 'data', file_name)\n",
    "    with open(csv_path, 'w') as csv_file: \n",
    "        # creating a csv dict writer object \n",
    "        print(\"Entries: {num}\".format(num=len(data)))\n",
    "        keys = data[0].keys()\n",
    "        writer = csv.DictWriter(csv_file, fieldnames = keys) \n",
    "        \n",
    "        # writing headers (field names) \n",
    "        writer.writeheader() \n",
    "        \n",
    "        # writing data rows \n",
    "        writer.writerows(data) \n",
    "        \n",
    "\n",
    "def read_csv(file_name=\"song_data.csv\"):\n",
    "    global final_, checked_artists     \n",
    "    \n",
    "    csv_path = os.path.join(os.path.curdir, 'data', file_name)\n",
    "    \n",
    "    # opening the CSV file\n",
    "    try:\n",
    "        with open(csv_path, mode ='r', encoding=\"UTF-8\") as file:   \n",
    "\n",
    "            # reading the CSV file\n",
    "            data = csv.DictReader(file)\n",
    "\n",
    "            for entry in data:\n",
    "                checked_artists.add(entry[\"artist\"])\n",
    "                final_.append(entry)\n",
    "                \n",
    "        print(\"Number of artists already found {num}\".format(num=len(checked_artists)))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "# Run genius search\n",
    "def search_genius(args):\n",
    "    import sys\n",
    "    artist_queue, num, genius, final_ = args\n",
    "    \n",
    "    def log(string):\n",
    "        print(\"[{num}] \".format(num=num) + string + \"\\n\", end='')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    # Processing\n",
    "    def clean_data(data):\n",
    "        cleaned_data = data.replace(\"\\n\", \" \").replace(\",\", \" \")\n",
    "        return cleaned_data\n",
    "\n",
    "    def process_artist(artist):\n",
    "        artist_dict = artist.to_dict()\n",
    "        return \"\"\n",
    "\n",
    "    def process_song(song):\n",
    "        lyrics = clean_data(song.lyrics)\n",
    "        return lyrics\n",
    "\n",
    "    def build_entry(artist, song, data, columns = [\"artist\", \"song\", \"data\"]):\n",
    "        entry = {\"artist\": artist, \"song\": song, \"data\": data}\n",
    "        return entry\n",
    "    \n",
    "    log(\"Starting\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            artist = artist_queue.get()\n",
    "            if artist is None:\n",
    "                log(\"Done\")\n",
    "                return\n",
    "            log(\"Searching {artist}\".format(num=num, artist=artist.strip()))\n",
    "            \n",
    "            # Pull data for artist from genius\n",
    "            genius_artist = genius.search_artist(artist, per_page=50, get_full_info=False)\n",
    "            \n",
    "            log(\"Finished {artist}\".format(num=num, artist=artist.strip()))\n",
    "            if genius_artist == None:\n",
    "                log(\"{artist} not found\".format(num=num, artist=artist.strip()))\n",
    "                continue\n",
    "                           \n",
    "            artist_data =  process_artist(genius_artist)\n",
    "                           \n",
    "            log(\"{artist} number of songs: {song_num}\".format(num=num, artist=artist.strip(), song_num=len(genius_artist.songs)))\n",
    "            \n",
    "            for song in genius_artist.songs:\n",
    "                song_data = process_song(song)\n",
    "                \n",
    "                # Add to final list\n",
    "                final_.append(build_entry(artist, song.title, song_data))\n",
    "    \n",
    "    except Exception as e:\n",
    "        log(\"Something went wrong: {error}\".format(num=num, error= e))\n",
    "    \n",
    "    \n",
    "def run(multi_core=False): \n",
    "    \n",
    "    # Setup Genius\n",
    "    genius = genius_setup()\n",
    "    \n",
    "    # Load in any previous data\n",
    "    print(\"Reading previous\")\n",
    "    read_csv()\n",
    "    \n",
    "    pool = None\n",
    "    try:  \n",
    "        if multi_core:\n",
    "            # multiprocess.log_to_stderr().setLevel(logging.DEBUG)\n",
    "            print(\"Multiprocessing with {process_number} processes\".format(process_number=process_number))\n",
    "            \n",
    "            artist_queue = multiprocess.Manager().Queue()\n",
    "            get_artists(artist_queue)\n",
    "            \n",
    "            for x in range(process_number):\n",
    "                artist_queue.put(None)\n",
    "            \n",
    "            print(artist_queue.qsize())\n",
    "            # creating processes\n",
    "            with multiprocess.get_context(\"spawn\").Pool(process_number) as pool:\n",
    "                args = [(artist_queue, x, genius, final_) for x in range(process_number)]\n",
    "                pool.map(search_genius, args)\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "            \n",
    "        else:\n",
    "            print(\"Running single core\")\n",
    "            artist_queue = queue.Queue()\n",
    "            get_artists(artist_queue)\n",
    "            artist_queue.put(None)\n",
    "            print(artist_queue.qsize())\n",
    "            search_genius((artist_queue, 0, genius, final_))\n",
    "\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        if pool:\n",
    "            pool.close()\n",
    "            pool.terminate()\n",
    "            pool.join()\n",
    "        print(\"KeyboardInterrupt: Writing results\")\n",
    "    \n",
    "    finally:\n",
    "        write_to_csv(list(final_))                       \n",
    "\n",
    "\n",
    "run(multi_core=True) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing \n",
    "Now we tokenize the lyrics into stemmed, lowercase unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metapy\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# OS agnostic\n",
    "import os \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data in song_data.csv\n",
      "Error decoding sonng Half The World Away but if it was released\n"
     ]
    }
   ],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\"\n",
    "    data: a string to tokenize\n",
    "    \n",
    "    tokens: a list of tokenized ngrams\n",
    "    \"\"\"\n",
    "    doc = metapy.index.Document()\n",
    "    doc.content(data)\n",
    "\n",
    "    tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "    tok = metapy.analyzers.LowercaseFilter(tok)\n",
    "    tok = metapy.analyzers.Porter2Filter(tok)               \n",
    "    ana = metapy.analyzers.NGramWordAnalyzer(1, tok)\n",
    "    trigrams = ana.analyze(doc)\n",
    "    tok.set_content(doc.content())\n",
    "\n",
    "    tokens, counts = [], []\n",
    "    for token, count in trigrams.items():\n",
    "        tokens.append(token)\n",
    "        counts.append(count)    \n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_file(input_file=\"song_data.csv\", output_file=\"song_data_tokenize.csv\"):\n",
    "    \"\"\"\n",
    "    processes a file (artist, song, data) into tokenized lyrics\n",
    "    currently only processes first song (?)\n",
    "    \n",
    "    file_name: file to tokenize data rows in\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing data in\", input_file)\n",
    "\n",
    "    # set our path and temp file to write to - safer\n",
    "    input_csv_path = os.path.join(os.path.curdir, 'data', input_file)\n",
    "    output_csv_path = os.path.join(os.path.curdir, 'data', output_file)\n",
    "    tempfile = NamedTemporaryFile('w+t', newline='', delete=False)\n",
    "    \n",
    "    # read in each lyric and tokenize it as a metapy document\n",
    "    try:\n",
    "        with open(input_csv_path, mode ='r+', encoding = 'utf-8') as file, tempfile:   \n",
    "            # read from main file, write to temp file\n",
    "            reader = csv.DictReader(file)\n",
    "            writer = csv.DictWriter(tempfile, extrasaction='ignore', \n",
    "                                    fieldnames=['artist', 'song', 'data'])\n",
    "            \n",
    "            for row in reader:\n",
    "                # print(\"Tokenizing song\", row['song'])\n",
    "                try:\n",
    "                    row['data'] = tokenize(row['data'])\n",
    "                    writer.writerow(row)\n",
    "                except UnicodeDecodeError:\n",
    "                    print(\"Error decoding sonng {}\".format(row['song']))\n",
    "            \n",
    "            shutil.move(tempfile.name, output_csv_path)\n",
    "    \n",
    "    except (FileNotFoundError) as err:\n",
    "        print(err)\n",
    "        \n",
    "        \n",
    "tokenize_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS410 Project Bae Area",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
