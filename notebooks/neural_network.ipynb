{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv(\"../src/data/recipes.csv\")\n",
    "reviews = pd.read_csv(\"../src/data/all_users.csv\")\n",
    "recipe_ids = pd.read_csv(\"../src/data/recipe_ids.csv\")\n",
    "# reviews = pd.read_csv(\"../src/data/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_user_ids = pd.DataFrame(reviews.user_id.unique()).reset_index()\n",
    "transformed_user_ids.columns = [\"transformed_user_id\",\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_recipe_ids = pd.DataFrame(reviews.recipe_id.unique()).reset_index()\n",
    "transformed_recipe_ids.columns = [\"transformed_recipe_id\",\"recipe_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.merge(transformed_user_ids, on=\"user_id\")\n",
    "reviews = reviews.merge(transformed_recipe_ids, on=\"recipe_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"transformed_rating\"] = reviews.rating - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vocab_size = reviews.transformed_user_id.max()\n",
    "item_vocab_size = reviews.transformed_recipe_id.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "# item_inputs = keras.Input(shape=(19,), name='items')\n",
    "# i = layers.Dense(64, activation='relu', name='item_dense_1')(item_inputs)\n",
    "# i = layers.Dropout(rate=0.2)(i)\n",
    "# i = layers.Dense(16, activation='relu', name='item_dense_2')(i)\n",
    "# i = keras.Model(inputs=item_inputs, outputs=i)\n",
    "\n",
    "item_embedding_layer = layers.Embedding(input_dim = item_vocab_size + 1, output_dim=128, input_length = None)\n",
    "item_id = keras.Input(shape=(1,), name='item_ids')\n",
    "item_embeddings = item_embedding_layer(item_id)\n",
    "item_embeddings = layers.Dropout(rate=0.3)(item_embeddings)\n",
    "item_embeddings = layers.Flatten()(item_embeddings)\n",
    "item_embeddings = keras.Model(inputs=item_id, outputs=item_embeddings)\n",
    "\n",
    "# user_inputs = keras.Input(shape=(3,), name='users')\n",
    "# u = layers.Dense(64, activation='relu', name='user_dense_1')(user_inputs)\n",
    "# u = layers.Dropout(rate=0.2)(u)\n",
    "# u = layers.Dense(32, activation='relu', name='user_dense_2')(u)\n",
    "# u = keras.Model(inputs=user_inputs, outputs=u)\n",
    "\n",
    "user_id = keras.Input(shape=(1,), name='user_ids')\n",
    "user_embeddings = layers.Embedding(input_dim = user_vocab_size + 1, output_dim=128, input_length = None)(user_id)\n",
    "user_embeddings = layers.Dropout(rate=0.3)(user_embeddings)\n",
    "user_embeddings = layers.Flatten()(user_embeddings)\n",
    "user_embeddings = keras.Model(inputs=user_id, outputs=user_embeddings)\n",
    "\n",
    "dot = layers.Dot(axes=1)([user_embeddings.output, item_embeddings.output])\n",
    "s = keras.Model(inputs=[user_id, item_id], outputs=dot)\n",
    "\n",
    "combined = layers.concatenate([user_embeddings.output, item_embeddings.output])\n",
    "z = layers.BatchNormalization(name=\"bn_top_0\")(combined)\n",
    "z = layers.Dense(32, activation='relu', name='top_combined_dense_1')(z)\n",
    "z = layers.Dropout(rate=0.2)(z)\n",
    "z = layers.BatchNormalization(name=\"bn_top_1\")(z)\n",
    "z = layers.Dense(16, activation='relu', name='top_combined_dense_2')(z)\n",
    "z = layers.concatenate([z, s.output])\n",
    "z = layers.Dense(5, name='predictions')(z)\n",
    "model = keras.Model(inputs=[user_id, item_id], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
    "              # List of metrics to monitor\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews.date < \"2018-01-01\"]\n",
    "test = reviews[reviews.date >= \"2018-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_train = train.transformed_user_id[:-1000]\n",
    "item_id_train = train.transformed_recipe_id[:-1000]\n",
    "y_train = train.transformed_rating[:-1000]\n",
    "\n",
    "user_id_test = test.transformed_user_id\n",
    "item_id_test = test.transformed_recipe_id\n",
    "y_test = test.transformed_rating\n",
    "\n",
    "user_id_val = train.transformed_user_id[-1000:]\n",
    "item_id_val = train.transformed_recipe_id[-1000:]\n",
    "y_val = train.transformed_rating[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     1.279805\n",
       "3     7.095766\n",
       "2    22.655011\n",
       "1    52.392505\n",
       "0    69.084525\n",
       "Name: transformed_rating, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (y_train.value_counts()/ y_train.value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 2s 74ms/step - loss: 6.1562 - accuracy: 0.2600 - val_loss: 1.5794 - val_accuracy: 0.3560\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 2s 68ms/step - loss: 5.1989 - accuracy: 0.3873 - val_loss: 1.5590 - val_accuracy: 0.4950\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 2s 68ms/step - loss: 4.6882 - accuracy: 0.4848 - val_loss: 1.5564 - val_accuracy: 0.3130\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 2s 69ms/step - loss: 4.0345 - accuracy: 0.5683 - val_loss: 1.5146 - val_accuracy: 0.4200\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 2s 69ms/step - loss: 3.2919 - accuracy: 0.6448 - val_loss: 1.3765 - val_accuracy: 0.7330\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 2s 68ms/step - loss: 2.5526 - accuracy: 0.7358 - val_loss: 1.1580 - val_accuracy: 0.7410\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 2s 69ms/step - loss: 1.9457 - accuracy: 0.8054 - val_loss: 1.0053 - val_accuracy: 0.7410\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 2s 69ms/step - loss: 1.4961 - accuracy: 0.8483 - val_loss: 0.9313 - val_accuracy: 0.7410\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 2s 69ms/step - loss: 1.2376 - accuracy: 0.8782 - val_loss: 0.9248 - val_accuracy: 0.7410\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 2s 69ms/step - loss: 1.0396 - accuracy: 0.8922 - val_loss: 0.9442 - val_accuracy: 0.7410\n",
      "\n",
      "history dict: {'loss': [6.156229496002197, 5.1989264488220215, 4.688233375549316, 4.0345354080200195, 3.291860342025757, 2.552598714828491, 1.9457457065582275, 1.4960805177688599, 1.2375755310058594, 1.0396062135696411], 'accuracy': [0.2599668800830841, 0.3872680068016052, 0.48477205634117126, 0.5683093070983887, 0.6448255181312561, 0.7357979416847229, 0.8054436445236206, 0.848304033279419, 0.8781576156616211, 0.892180860042572], 'val_loss': [1.5793732404708862, 1.5590441226959229, 1.556445598602295, 1.5145902633666992, 1.376549482345581, 1.1580214500427246, 1.0053386688232422, 0.9313068985939026, 0.9248469471931458, 0.9442068934440613], 'val_accuracy': [0.35600000619888306, 0.4950000047683716, 0.31299999356269836, 0.41999998688697815, 0.7329999804496765, 0.7409999966621399, 0.7409999966621399, 0.7409999966621399, 0.7409999966621399, 0.7409999966621399]}\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "# del model\n",
    "history = model.fit([user_id_train, item_id_train], y_train,\n",
    "                    batch_size=2000,\n",
    "                    epochs=10,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # Introduce loss weights to handle class imbalance\n",
    "#                     callbacks=[callback],\n",
    "                    class_weight= {\n",
    "                        0: 37,\n",
    "                        1: 36,\n",
    "                        2: 17,\n",
    "                        3: 5,\n",
    "                        4: 1\n",
    "                    },\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=([user_id_val, item_id_val], y_val)\n",
    "                   )\n",
    "print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on train data\n",
      "train loss, train accuracy: [0.6205962300300598, 0.7813687920570374]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on train data')\n",
    "results = model.evaluate([user_id_train, item_id_train], y_train, batch_size=y_train.shape[0], verbose=0)\n",
    "print('train loss, train accuracy:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.7813688212927756\n"
     ]
    }
   ],
   "source": [
    "# Verify with sklearn metrics\n",
    "logits = model.predict([user_id_train, item_id_train])\n",
    "proba = softmax(logits, axis=1)\n",
    "predictions = np.argmax(proba,axis=1)\n",
    "print(\"train accuracy:\",accuracy_score(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 0.849041304380012\n"
     ]
    }
   ],
   "source": [
    "print(\"train rmse:\", math.sqrt(mean_squared_error(y_train, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "test loss, test accuracy: [0.7539902925491333, 0.8062964081764221]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate([user_id_test, item_id_test], y_test, batch_size=y_test.shape[0], verbose=0)\n",
    "print('test loss, test accuracy:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8062964125091517\n"
     ]
    }
   ],
   "source": [
    "# Verify with sklearn metrics\n",
    "logits = model.predict([user_id_test, item_id_test])\n",
    "proba = softmax(logits, axis=1)\n",
    "predictions = np.argmax(proba,axis=1)\n",
    "print(\"test accuracy:\",accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse: 0.7704944846414414\n"
     ]
    }
   ],
   "source": [
    "print(\"test rmse:\", math.sqrt(mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
