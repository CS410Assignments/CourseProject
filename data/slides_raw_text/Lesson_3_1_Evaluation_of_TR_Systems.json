{
    "0": "Evaluation of Text Retrieval Systems\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Course Schedule\n \n2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n11. \nRecommendation\n \n3. Text Retrieval Problem\n \n10. \nWeb Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n7\n. \nEvaluation \n \n6. System \n \nImplementation\n \n5\n. Vector Space Model\n \n8\n. Probabilistic Model \n \n9. Feedback  \n \nEvaluation of Text Retrieval  Systems\n \n",
    "2": "Why Evaluation?\n \n\nReason 1: A\nssess the actual utility of a TR system\n \n\nMeasures should reflect the utility to users in a real application \n \n\nUsually done through user studies (interactive IR evaluation)\n \n\nReason 2: C\nompare \ndifferent systems and \nmethods\n \n\nMeasures \nonly need to be correlated with the utility to actual \n\nusers \n \n\nUsually \ndone through test collections (test set IR evaluation)\n \n \n3\n \n",
    "3": "4\n \nWhat to Measure? \n \n\nEffectiveness/Accuracy: how accurate are the search results? \n \n\n\nnon\n-\nrelevant ones\n \n\nEfficiency: how quickly can a user get the results? How much \ncomputing resources are needed to answer a query? \n \n\nMeasuring space and time overhead\n \n\nUsability: How useful is the system for real user tasks?\n \n\nDoing user studies  \n \n",
    "4": "5\n \nThe Cranfield Evaluation Methodology\n \n\nA methodology for laboratory testing of system components developed in \n1960s\n \n\nIdea: Build \nreusable\n \ntest collections & define measures \n \n\nA sample collection of documents (simulate real document collection)\n \n\nA sample set of queries/topics (simulate user queries) \n \n\nRelevance judgments (ideally made by users who formulated the queries) \n\n \nIdeal \nranked list\n \n\n\n \n\nA test collection can then be reused many times to compare different systems\n \n",
    "5": "Test Collection Evaluation \n \n6\n \nQ1  D1  +\n \nQ1  D2  +\n \nQ1  D3 \n\n \nQ1  D4 \n\n \nQ1  D5 +\n \n\n \nQ2  D1 \n\n \nQ2  D2 +\n \nQ2  D3 +\n \nQ2  D4 \n\n \n\n \nQ50 D1 \n\n \nQ50 D2 \n\n \nQ50 D3 +\n \n\n \nRelevance \n \nJudgments\n \nDocument Collection\n \nQ1  Q2 Q3\n \n\n \n \nD1\n \nD2\n \nD3\n \nD48\n \n\n \nQueries\n \nD2 +\n \nD1 + \n \nD4  \n-\n \n \nSystem A\n \nSystem B\n \nQuery= Q1\n \nD1 +\n \nD4  \n-\n \nD3  \n-\n \n \nD5 +\n \nD2 +\n \nR\nA\n \nR\nB\n \nWhich  is better?\n \nR\nA \nor R\nB\n?\n \nHow to quantify? \n \n"
}