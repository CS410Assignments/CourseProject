{
    "0": " \n \n\nZhai\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \nText Retrieval and Search Engines\n \nVector Space Model: Doc Length Normalization\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Course Schedule\n \n2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n11. Recommendation\n \n3. Text Retrieval Problem\n \n10. Web Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n7. Evaluation \n \n6. System \n \nImplementation\n \n5. Vector Space Model\n \n8. Probabilistic Model \n \n9. Feedback  \n \n",
    "2": "What about Document Length?  \n \n\n \n\nnews\n \nof \npresidential campaign \n\n \n\npresidential \n\n \nd4\n \n\ncampaign\n\n \ncampaign\n................................................... \n\n \n...........\nnews\n.................................................................................\n \n\n\nnews\n\n\n\n \n\n \n\npresidential\n \n\n \npresidential\n\n \n \nd6\n \n  \nd6 > d4?\n \n100 words\n \n5000 words\n \n",
    "3": "Document Length Normalization\n \n\nP\nenalize a long doc with a doc length normalizer \n \n\nLong doc has a better chance to match any query \n \n\nNeed to avoid over\n-\npenalization\n \n\nA document is long because \n \n\nit \nuses more \nwords \n\n \nmore penalization\n \n \n\nit \nhas more \ncontents \n\n \nless penalization\n \n\nPivoted length normalizer: average doc length as \n\n \n\nNormalizer = 1 if |d| =average doc length (\navdl\n)\n \n \n \n \n4\n \n",
    "4": "Pivoted Length Normalization\n \n5\n \n \n|d|\n \n\navdl\n        \n\n \n1.0\n \nb=0\n \nb>0\n \nb>>0\n \nLonger than \navdl\n \n \nShorter than \navdl\n \n \nPenalization\n \nReward\n \n",
    "5": "State of the Art VSM Ranking Functions \n \n\nPivoted Length Normalization VSM [\nSinghal\n \net al 96]\n \n \n \n \n\nBM25/Okapi [Robertson & Walker 94] \n \n6\n \n",
    "6": "Further Improvement of VSM?\n \n\nImproved instantiation of \ndimension?\n \n\nstemmed words, stop word removal, phrases, latent semantic \nindexing (word clusters), character n\n-\n\n \n\nbag\n-\nof\n-\nwords with phrases is often sufficient in practice\n \n\nLanguage\n-\nspecific and domain\n-\nspecific tokenization is important to \n\n \n\nImproved  instantiation of \nsimilarity function?\n \n\ncosine of angle between two vectors?\n \n\nEuclidean? \n \n\ndot product seems still the best (sufficiently general especially with \nappropriate term weighting)\n \n \n7\n \n",
    "7": "Further Improvement of BM25\n \n\nBM25F [Robertson & Zaragoza 09]\n \n\n\n \n\nKey idea: combine the frequency counts of terms in all fields \nand then apply BM25 (instead of the other way)\n \n\nBM25+ [\nLv\n \n& \nZhai\n \n11] \n \n\nAddress the problem of over penalization of long documents \nby BM25 by adding a small constant to TF\n \n\nEmpirically and \nanalytically\n \nshown to be better than BM25 \n \n8\n \n",
    "8": "Summary of Vector Space Model \n \n\nRelevance(\nq,d\n) = similarity(\nq,d\n)\n \n\nQuery and documents are represented as vectors\n \n\nHeuristic design of ranking function \n \n\nMajor term weighting heuristics\n \n\nTF weighting and transformation\n \n\nIDF weighting \n \n\nDocument length normalization\n \n\nBM25 and Pivoted normalization seem to be most \neffective\n \n \n9\n \n",
    "9": "Additional Readings\n \n\nA. \nSinghal\n, \nC. \nBuckley, and \nM. \nMitra\n. Pivoted \ndocument length \nnormalization. In \nProceedings \nof ACM SIGIR 1996. \n \n\nS. E. Robertson and S. Walker. Some simple effective \napproximations to the 2\n-\nPoisson model for probabilistic \nweighted retrieval, \nProceedings of ACM SIGIR 1994\n.\n \n\nS. Robertson and H. Zaragoza. \nThe Probabilistic Relevance \nFramework: BM25 and \nBeyond, \nFound. Trends Inf. \nRetr\n.\n \n3, 4 \n(April 2009\n). \n \n\nY. \nLv\n, C. \nZhai\n, \nLower\n-\nbounding term frequency normalization. \nIn \nProceedings of \nACM CIKM 2011.\n \n10\n \n"
}