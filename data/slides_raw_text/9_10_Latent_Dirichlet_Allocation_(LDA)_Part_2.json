{
    "0": "Latent \nDirichlet\n \nAllocation (LDA) \n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Latent \nDirichlet\n \nAllocation (LDA)\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "Extensions of PLSA\n \n\nPLSA with prior knowledge \n\n \nUser\n-\ncontrolled PLSA\n \n\nPLSA as a generative model \n\n \nLatent \nDirichlet\n \nAllocation\n \n \n \n3\n \n",
    "3": "PLSA with Prior Knowledge\n \n\nUsers may have expectations about which topics to analyze:\n \n\n\n \n\n\nabout a laptop\n \n\nUsers may have knowledge about what topics are (or are NOT) \ncovered in a document\n \n\nTags = topics \n\n \nA doc can only be generated using topics \ncorresponding to the tags assigned to the document \n \n\nWe can incorporate such knowledge as priors of PLSA model\n \n4\n \n",
    "4": "Maximum a Posteriori (MAP) Estimate \n \n\nWe may use p(\n\n) to encode all kinds of preferences and \nconstraints, e.g., \n \n\np(\n\n\n\nB\n)\n \n\np(\n\n)>0 if and only if \nfor a particular doc d, \n\nd,3\n=0\n \nand \n\nd,1\n=1/2\n \n\np(\n\n) favors a \n\n \nwith topics that assign high probabilities to some \nparticular words \n \n\nThe MAP estimate (with conjugate prior) can be computed \nusing a similar EM algorithm to the ML estimate with \nsmoothing to reflect prior preferences\n \n \n \n \n5\n \n",
    "5": "EM Algorithm with Conjugate Prior on p(w|\n \n\ni\n)\n \n+\n\np(w|\n\n\nj\n)\n \n+\n\n \nPseudo counts of \nw\n \n \nfrom prior \n\n\n \nSum of all pseudo counts\n \nWhat if \n\n=0? What if \n\n=+\n\n?\n \nbattery 0.5 \n \nlife  0.5\n \nPrior: p(w\n|\n\n\nj\n)\n \nWe may also set any parameter to a constant (including 0) as needed\n \n6\n \n",
    "6": "Deficiency of PLSA \n \n\nNot a generative model\n \n\nC\n\n \n\nHeuristic workaround is possible, though\n \n\nMany parameters \n\n \nhigh complexity of models\n \n\nMany local maxima  \n  \n \n\nProne to \noverfitting\n \n \n\nNot \nnecessarily \na problem for text mining (only interested \n\n \n \n7\n \n",
    "7": "Latent \nDirichlet\n \nAllocation (LDA)\n \n\nMake PLSA a generative model by imposing a \nDirichlet\n \nprior on the model parameters \n\n \n\nLDA = Bayesian version of PLSA\n \n\nParameters are regularized \n \n\nCan achieve the same goal as PLSA for text mining \npurposes \n \n\nTopic coverage and topic word distributions can be inferred \nusing Bayesian inference \n \n \n8\n \n8\n \n",
    "8": "PLSA \n\n \nLDA\n \nTopic  \n\n1\n \nTopic \n\nk\n \nTopic \n\n2\n \ngovernment 0.3 \n \nresponse  0.2\n \n...\n \ndonate  0.1\n \nrelief 0.05\n \nhelp 0.02 \n \n...\n \ncity 0.2\n \nnew   0.1\n \norleans\n \n0.05 \n \n...\n \nBoth word distributions and \n \nt\nopic choices are free in PLSA\n \np\n(\n\n1\n)=\n\nd,1\n \n \np\n(\n\n2\n)=\n\nd,2\n \n \np\n(\n\nk\n)=\n\nd,k\n \n \n\n \np(w|\n\n1\n)\n \np(w|\n\n2\n)\n \np(w|\n\nk\n)\n \n\n \nw\n \nLDA imposes a prior on both\n \n9\n \n",
    "9": "Likelihood Functions for PLSA vs. LDA\n \nPLSA\n \nLDA\n \nCore assumption \n \nin all topic models\n \nPLSA component\n \nAdded by LDA\n \n10\n \n",
    "10": "Parameter Estimation and Inferences in LDA\n \n\nParameters can \nbe \nestimated using ML estimator\n \n \n \n \n\nHowever\n, \n{\n\nj\n} and {\n\nd,j\n}\n \nmust now be computed using \nposterior \ninference\n \n\nComputationally intractable\n \n\nMust resort to approximate inference \n \n\nMany different inference methods are available \n \n \n \nHow many parameters in LDA vs. PLSA?\n \n11\n \n",
    "11": "Summary of Probabilistic Topic Models\n \n\nProbabilistic topic models provide a general principled way of \nmining and analyzing topics in text with many applications\n \n\nBasic task setup: \n \n\nInput: Text data \n \n\nOutput: k topics + proportions of these topics covered in each \ndocument \n \n\nPLSA is the basic topic model, often adequate for most \napplications \n \n\nLDA improves over PLSA by imposing priors\n \n\nTheoretically more appealing\n \n\nPractically, LDA and PLSA perform similarly for many tasks  \n \n12\n \n",
    "12": "Suggested Readings\n \n\nBlei\n\nCommunications of \nthe ACM\n \n55 (4): 77\n\n84. \ndoi\n: \n10.1145/2133806.2133826.\n \n\nQiaozhu\n \nMei, \nXuehua\n \nShen\n, and \nChengXiang\n \nZhai\n.\n \n\nL\nabeling \nof \nMultinomial \nT\nopic \nM\n\nProceedings of ACM KDD \n2007\n, \npp. 490\n-\n499\n, \nDOI=10.1145/1281192.1281246.\n \n\nYue Lu, \nQiaozhu\n \nMei, and \nChengxiang\n \nZhai\n. 2011. Investigating \ntask performance of probabilistic topic models: an empirical \nstudy of PLSA and LDA. \nInformation Retrieval\n, 14\n, 2 (April 2011), \n178\n-\n203. \nDOI=10.1007/s10791\n-\n010\n-\n9141\n-\n9.\n \n13\n \n"
}