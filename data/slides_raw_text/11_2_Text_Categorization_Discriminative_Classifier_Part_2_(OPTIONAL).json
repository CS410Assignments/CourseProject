{
    "0": "Text Categorization: \n \nDiscriminative Classifiers (Part 2)\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Discriminative Classifier 3: Support Vector Machine (SVM)\n \n\nConsider two categories: {\n\n1\n, \n\n2\n}\n \n\nUse a linear separator\n \n2\n \nX\n1\n \nX\n2\n \nAssume \n\n1\n<0, \n\n2\n>0\n \n",
    "2": "Which Linear \nS\neparator \nI\ns the Best? \n \n3\n \nX\n1\n \nX\n2\n \n",
    "3": "Best Separator = Maximize the Margin\n \n4\n \nX\n1\n \nX\n2\n \nMargin\n \nMargin\n \nw\nT\nx+\nb\n=\n0\n \nFeature Vector\n \n \n \n(e.g., word counts)\n \nFeature\n \nWeights\n \nBias constant\n \nNotation Change: \n\n\nw; \n\n0\n\nb\n \n \n",
    "4": "Only the Support Vectors Matter\n \n5\n \nX\n1\n \nX\n2\n \nSupport Vectors\n \nSupport Vectors\n \nw\nT\nx+\nb\n=\n0\n \n",
    "5": "Linear SVM\n \n6\n \nClassifier\n: f(x)=w\nT\nx+\nb\n \nParameters\n: \nw, \nb\n \nTraining Data: \nT={(\nx\ni\n, \ny\ni\n\n \nx\ni \nis a feature vector; \ny\ni\n \n\n{\n-\n1, 1}\n \nGoal 1: Correct labeling on training data:  \n \nIf y\ni\n=1 \n\n \n \nw\nT\nx\ni\n+\nb\n \n\n1\n \nIf y\ni\n=\n-\n1 \n\n \n \nw\nT\nx\ni\n+\nb\n \n\n \n-\n1 \n \n\ni, \ny\ni\n(\nw\nT\nx\ni\n+\nb\n)\n\n1\n \nConstraint\n \nObjective\n \nMinimize \n\n(w)=\nw\nT\nw\n \nGoal 2: Maximize margin\n \nLarge margin \n\n \nSmall \nw\nT\nw\n \nThe optimization problem is quadratic programming with linear constraints \n \n",
    "6": "Linear SVM with Soft Margin\n \n7\n \n\ni\n\n[1,|T|], \ny\ni\n(\nw\nT\nx\ni\n+\nb\n)\n\n1\n-\n\ni\n,    \n\ni\n\n0\n \n\n(w)=\nw\nT\nw+\nC\n\ni\n\n[1,|T|]\n\ni\n \nThe optimization problem is still quadratic programming with linear constraints \n \nTraining Data: \nT={(\nx\ni\n, \ny\ni\n\n \nFind w, b, and \n\ni\n \nto minimize\n \nSubject to\n \nC>0 is a parameter to control the trade\n-\noff  between \n \nminimizing the errors and maximizing the margin\n \nClassifier\n: f(x)=w\nT\nx+\nb >0?\n \nParameters\n: \nw, \nb\n \nAdded to allow training errors\n \n",
    "7": "Summary of Text Categorization Methods\n \n\nMany methods are available, but no clear winner\n \n\nA\nll require effective feature representation (need domain knowledge) \n \n\nIt is useful to compare/combine multiple methods for a particular \nproblem\n \n\nMost techniques rely on supervised machine learning\n \nand thus \ncan be applied to \nany\n \ntext categorization problem!\n \n\nHumans annotate training data and design features \n \n\nComputer optimizes the combination of features \n \n\nGood performance requires 1) effective features and 2) plenty of \ntraining data\n \n\nPerformance is generally (much) more affected by the effectiveness of \nfeatures than by the choice of a specific classifier\n \n \n \n8\n \n",
    "8": "Summary of Text Categorization Methods (cont.)\n \n\nHow to design effective features?  (application\n-\nspecific)\n \n\nAnalyze the categorization problem and exploit domain knowledge\n \n\nPerform error analysis to obtain insights \n \n\nLeverage machine learning techniques (e.g., feature selection, \ndimension reduction, deep learning)\n \n\nHow to obtain \n\nexamples?  \n \n\nLow\n-\n\n \n\nExploit unlabeled data \n(using semi\n-\nsupervised learning techniques)\n \n\n\nfrom a related domain/problem)\n \n \n \n9\n \n",
    "9": "Suggested Reading\n \nManning, Chris D., \nPrabhakar\n \nRaghavan\n, and \nHinrich\n \nSch\u00fctze\n. \nIntroduction to Information Retrieval\n. \nCambridge: Cambridge University Press, 2007.  \n(\nChapters 13\n-\n15)\n \n \n \n10\n \n"
}