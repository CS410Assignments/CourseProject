{
    "0": "Natural Language Content Analysis\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Natural Language Content Analysis\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n2. Word association \nmining  & analysis\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining & \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining & analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n",
    "2": "Basic Concepts in NLP\n \nA \n  \ndog   is   chasing   a   boy   on   the   playground\n \nDet\n \nNoun\n \nAux\n \nVerb\n \nDet\n \nNoun\n \nPrep\n \nDet\n \nNoun\n \nNoun Phrase\n \nComplex Verb\n \nNoun Phrase\n \nNoun Phrase\n \nPrep Phrase\n \nVerb Phrase\n \nVerb Phrase\n \nSentence\n \nDog(d1).\n \nBoy(b1).\n \nPlayground(p1).\n \nChasing(d1,b1,p1).\n \nSemantic \n \nanalysis\n \nLexical\n \nanalysis\n \n(part\n-\nof\n-\nspeech\n \ntagging)\n \nSyntactic analysis\n \n(Parsing)\n \nA person saying this may\n \nbe reminding another person to\n \nget the dog \nback.\n \nPragmatic analysis\n \n(speech act)\n \nScared(x) if Chasing(_,x,_).\n \n+\n \nScared(b1)\n \nInference\n \n3\n \n",
    "3": "4\n \nNLP Is Difficult!\n \n\nNatural language is designed to make human communication \nefficient. As a result,\n \n\nwe omit a lot of \ncommon sense \nknowledge, which we \nassume the hearer/reader possesses.\n \n\nwe keep a lot of ambiguities, which we assume the \nhearer/reader knows how to resolve.\n \n\nThis makes EVERY step in NLP hard\n \n\nAmbiguity is a \nkiller\n!\n \n\nCommon sense reasoning is pre\n-\nrequired.\n \n \n",
    "4": "5\n \nExamples of Challenges\n \n\nWord\n-\nlevel ambiguity:\n \n\n\n \n(ambiguous POS)  \n \n\n\n \n(ambiguous sense)\n \n\nSyntactic ambiguity:\n \n\n\n(modification)\n \n\n\nwith a telescope\n\n \n(PP Attachment)\n \n\nAnaphora resolution: \n\nhimself\n\n(himself = John or Bill?)\n \n\nPresupposition: \n\n\nimplies that he smoked \nbefore.\n \n",
    "5": " \nThe State of the Art \n \n6\n \nA   dog   is   chasing   a   boy   on   the   playground\n \nDet\n \nNoun\n \nAux\n \nVerb\n \nDet\n \nNoun\n \nPrep\n \nDet\n \nNoun\n \nNoun Phrase\n \nComplex Verb\n \nNoun Phrase\n \nNoun Phrase\n \nPrep Phrase\n \nVerb Phrase\n \nVerb Phrase\n \nSentence\n \nSemantics: some aspects\n \n \n-\n \nEntity/relation extraction\n \n-\n \nWord sense disambiguation\n \n-\n \nSentiment analysis\n \nPOS\n \nTagging:\n \n97%\n \nParsing: partial >90%(?) \n \nSpeech act analysis: ???\n \nInference: ???\n \n",
    "6": "7\n \n\n \n\n100% POS tagging\n \n\n\noff\n \n\nvs\n \n\noff \n\n \n\nGeneral complete parsing\n \n\n\n \n\nPrecise deep semantic analysis\n \n\n\na \n\n \n \n \nRobust  \nand \ngeneral NLP tends to be \nshallow\n \nwhile \ndeep\n \n\nup. \n \n",
    "7": "Summary\n \n\nNLP is the foundation for text mining\n \n\nComputers are far from being able to understand natural \nlanguage \n \n\nDeep NLP requires common sense knowledge and inferences, \nthus only working for very limited domains\n \n\nShallow NLP based on statistical methods can be done in large \nscale and is thus more broadly applicable \n \n\nIn practice: statistical NLP as the basis, while humans \nprovide help as needed\n \n \n \n8\n \n",
    "8": "Additional Reading\n \nManning, Chris \nand \nHinrich\n \nSch\u00fctze\n.\n \nFoundations of \nStatistical Natural Language \nProcessing\n. Cambridge: \nMIT \nPress, \n1999.\n \n \n9\n \n"
}