{
    "0": "Text Clustering: \n \nGenerative Probabilistic Models (Part 2)\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Text Clustering: Generative Probabilistic Models \n(Part 2)\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "3\n \nd=x\n1\n \nx\n2 \n\nx\nL\n \nHow can we generalize it to include k topics/clusters?  \n \nLikelihood Function: p(d)=? \n \n",
    "3": "Mixture Model for Document Clustering\n \n\nData: a collection of documents C={d\n1\n\nd\nN\n}\n \n\nModel: mixture of k unigram LMs: \n\n=(\n{\n\ni\n}; {p(\n\ni\n)}), \ni\n\n[1,k]\n \n\nTo generate a document, first \nchoose a \n\ni\n \naccording to  \np(\n\ni\n), and then \ngenerate \nall\n \nwords in the document using p(w|\n\ni\n)\n \n\nLikelihood: \n \n \n \n \n\nMaximum Likelihood estimate\n \n4\n \n",
    "4": "Cluster Allocation After Parameter Estimation\n \n\nParameters\n \nof the mixture model: \n\n=(\n{\n\ni\n}; {p(\n\ni\n)}), \ni\n\n[1,k]\n \n\nEach \n\ni\n \nrepresents the \ncontent of cluster \ni\n \n: \np(w|\n \n\ni\n)\n \n\np(\n\ni\n) \nindicates the \nsize of cluster \ni\n \n \n\nNote that unlike in PLSA, p\n(\n\ni\n\n \n\nWhich cluster should document d belong to? c\nd\n=? \n \n\nLikelihood only\n: Assign d to the cluster corresponding to the \ntopic \n\ni\n \n \nthat\n \nmost likely has been used to generate d\n \n \n\nLikelihood + prior \np(\n\ni\n) (Bayesian): \nfavor large clusters\n \n \n5\n \n"
}