{
    "0": "Retrieval Methods: Feedback in Text Retrieval\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n8. Recommendation\n \n3. Text Retrieval Problem\n \n7. Web Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n5. Evaluation \n \n6. System \n \nImplementation\n \n4.3 Feedback in TR\n \nText Retrieval Methods:  Feedback in TR\n \n",
    "2": "3\n \nRelevance Feedback\n \nUpdated\n \nquery\n \nFeedback\n \nJudgments:\n \nd\n1 \n+\n \nd\n2\n \n-\n \nd\n3 +\n \n\n \nd\nk\n  \n-\n \n...\n \nQuery\n \nRetrieval\n \nEngine\n \nResults:\n \nd\n1 \n3.5\n \nd\n2\n \n2.4\n \n\n \nd\nk\n  \n0.5\n \n...\n \nUser\n \nDocument\n \ncollection\n \nUsers make explicit relevance judgments on the initial results\n \n\n \n",
    "3": "4\n \nPseudo/Blind/Automatic Feedback\n \nQuery\n \nRetrieval\n \nEngine\n \nResults:\n \nd\n1 \n3.5\n \nd\n2\n \n2.4\n \n\n \nd\nk\n  \n0.5\n \n...\n \nJudgments:\n \nd\n1 \n+\n \nd\n2\n \n+\n \nd\n3 +\n \n\n \nd\nk\n  \n-\n \n...\n \nDocument\n \ncollection\n \nFeedback\n \nUpdated\n \nquery\n \n \ntop 10 \n \nassumed \n \nrelevant\n \nTop\n-\nk initial results are simply assumed to be relevant\n \n\n \n",
    "4": "5\n \nImplicit Feedback\n \nUpdated\n \nquery\n \nFeedback\n \nClickthroughs\n:\n \nd\n1 \n+\n \nd\n2\n \n-\n \nd\n3 +\n \n\n \nd\nk\n  \n-\n \n...\n \nQuery\n \nRetrieval\n \nEngine\n \nResults:\n \nd\n1 \n3.5\n \nd\n2\n \n2.4\n \n\n \nd\nk\n  \n0.5\n \n...\n \nUser\n \nDocument\n \ncollection\n \nUser\n-\nclicked docs are assumed to be relevant; skipped ones non\n-\nrelevant \n \n\n \n"
}