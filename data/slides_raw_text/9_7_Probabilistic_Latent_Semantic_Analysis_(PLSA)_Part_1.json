{
    "0": "Probabilistic Latent Semantic Analysis (PLSA) \n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Probabilistic Latent Semantic Analysis (PLSA)\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "Document as a Sample of Mixed  Topics\n \nTopic  \n\n1\n \nTopic \n\nk\n \nTopic \n\n2\n \n\n \nBackground \n\nB\n \n \ngovernment 0.3 \n \nresponse  0.2\n \n...\n \ndonate  0.1\n \nrelief 0.05\n \nhelp 0.02 \n \n...\n \ncity 0.2\n \nnew   0.1\n \norleans\n \n0.05 \n \n...\n \nthe  \n0.04\n \na 0.03 \n \n...\n \n[ \nCriticism \nof\n \ngovernment response \nto the \nhurricane primarily consisted \nof \ncriticism \nof its \nresponse \nto the \napproach \nof the \nstorm \nand its \naftermath, specifically \nin the \ndelayed response\n \n]\n \nto the \n[\n \n\n \nof the \n1.3 million residents \nof the \ngreater New Orleans \nmetropolitan area evacuated\n \n]\n \n\n[ Over seventy \ncountries pledged monetary donations \nor other\n \nassistance]\n\n \n\n \nMany applications are possible if we can \n \n\n \n3\n \n",
    "3": "Mining Multiple Topics from Text\n \n4\n \n\n \nDoc 2\n \nDoc N\n \n\n \nDoc 1\n \n\n1\n \n\n2\n \n\nk\n \n\n11\n \n\n12\n \n\n1k\n \n\n21\n=0%\n \n\n2\n2\n \n\n2\nk\n \n\nN1\n=0%\n \n \n\nN\n2\n \n\nN\nk\n \n30%\n \n1\n2\n%\n \n8\n%\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \n\n \ntravel  0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \n\n \nINPUT:  C, k, V\n \nOUTPUT: { \n\n1\n\n\nk\n \n}, \n{ \n\ni1\n\n\nik\n \n}\n \nText Data\n \n",
    "4": "Generating Text with Multiple Topics: p(w)=?\n \nTopic  \n\n1\n \nTopic \n\nk\n \nTopic \n\n2\n \nBackground \n\nB\n \n \ngovernment 0.3 \n \nresponse  0.2\n \n...\n \ndonate  0.1\n \nrelief 0.05\n \nhelp 0.02 \n \n...\n \ncity 0.2\n \nnew   0.1\n \norleans\n \n0.05 \n \n...\n \nthe  \n0.04\n \na 0.03 \n \n...\n \nTopic \n \nChoice\n \np\n(\n\nB\n)=\n \n\nB\n \np\n(\n\n1\n)=\n\nd,1\n \n \np\n(\n\n2\n)=\n\nd,2\n \n \np\n(\n\nk\n)=\n\nd,k\n \n \n\n \np(w|\n\n1\n)\n \np(w|\n\n2\n)\n \np(w|\n\nk\n)\n \np(w|\n\nB\n)\n \n\n \nw\n \n1\n-\n \n\nB\n \n\nB \np(w|\n\nB\n)\n \n(1\n-\n \n\nB\n)p\n(\n\nk\n) p(w|\n\nk\n)\n \n(1\n-\n\nB\n)p\n(\n\n2\n) p(w|\n\n2\n)\n \n(1\n-\n\nB\n)p\n(\n\n1\n) p(w|\n\n1\n)\n \n+\n \n+\n \n\n \n+\n \n+\n \n5\n \n",
    "5": "Probabilistic Latent Semantic Analysis (PLSA)  \n \nUnknown Parameters\n: \n\n=({\n\nd,j\n}, {\n\n \nj\n\n \nPercentage of \n \nbackground words\n \n(known)\n \nBackground\n \nLM (known)\n \nCoverage of topic \n\n \nj \n \nin doc d\n \nProb. of word w in topic \n\n \nj \n \nHow many unknown parameters are there in total?\n \n6\n \n",
    "6": "ML Parameter Estimation  \n \nConstrained Optimization:\n \n7\n \n",
    "7": "EM Algorithm for PLSA: E\n-\nStep \n \nProbability that \nw in doc d \nis generated from \ntopic \n\n \nj \n \nProbability that \nw in doc d \nis generated  from \nbackground \n\n \nB\n \n \nUse of Bayes Rule\n \nHidden Variable (=topic indicator):  \nz\nd,w\n \n\n\n \n8\n \n",
    "8": "EM Algorithm for PLSA: M\n-\nStep \n \nRe\n-\nestimated \nprobability\n \nof  \ndoc d \ncovering \ntopic \n\n \nj \n \nRe\n-\nestimated \nprobability\n \nof \nword w \nfor\n \ntopic \n\n \nj\n \n \nML Estimate based on \n\ncounts to topic \n\n \nj \n \nHidden Variable (=topic indicator):  \nz\nd,w\n \n\n\n \n9\n \n",
    "9": "Computation of the EM Algorithm\n \n\nInitialize all unknown parameters randomly\n \n\nRepeat until likelihood converges \n \n\nE\n-\nstep \n \n \n\nM\n-\nstep\n \n10\n \n\n \n \nfor this one? \n \nIn general, accumulate counts, and then normalize\n \n",
    "10": "Summary\n \n\nPLSA = mixture model with k unigram LMs (k topics)\n \n\nAdding a pre\n-\ndetermined background LM helps discover \ndiscriminative topics \n \n\n\n \n\nk word distributions (k topics)\n \n\np\nroportion of each topic in each document \n \n\nThe output can enable many applications! \n \n\nClustering of terms and docs (treat each topic as a cluster)\n \n\nFurther associate topics with different contexts (e.g., time periods, \nlocations, authors, sources, etc.)\n \n11\n \n"
}