{
    "0": "Evaluation of TR Systems: Basic Measures\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Evaluation of TR Systems: Basic Measures\n \n2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n11. \nRecommendation\n \n3. Text Retrieval Problem\n \n10. \nWeb Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n7\n. \nEvaluation \n \n6. System \n \nImplementation\n \n5\n. Vector Space Model\n \n8\n. Probabilistic Model \n \n9. Feedback  \n \n",
    "2": "Test Collection Evaluation \n \n3\n \nQ1  D1  +\n \nQ1  D2  +\n \nQ1  D3 \n\n \nQ1  D4 \n\n \nQ1  D5 +\n \n\n \nQ2  D1 \n\n \nQ2  D2 +\n \nQ2  D3 +\n \nQ2  D4 \n\n \n\n \nQ50 D1 \n\n \nQ50 D2 \n\n \nQ50 D3 +\n \n\n \nRelevance \n \nJudgments\n \nDocument Collection\n \nQ1  Q2 Q3\n \n\n \n \nD1\n \nD2\n \nD3\n \nD48\n \n\n \nQueries\n \nD2 +\n \nD1 + \n \nD4  \n-\n \n \nSystem A\n \nSystem B\n \nQuery= Q1\n \nD1 +\n \nD4  \n-\n \nD3  \n-\n \n \nD5 +\n \nD2 +\n \nR\nA\n \nR\nB\n \nWhich  is better?\n \nR\nA \nor R\nB\n?\n \nHow to quantify? \n \n",
    "3": "Test Collection Evaluation \n \n4\n \nQ1  D1  +\n \nQ1  D2  +\n \nQ1  D3 \n\n \nQ1  D4 \n\n \nQ1  D5 +\n \n\n \nQ2  D1 \n\n \nQ2  D2 +\n \nQ2  D3 +\n \nQ2  D4 \n\n \n\n \nQ50 D1 \n\n \nQ50 D2 \n\n \nQ50 D3 +\n \n\n \nRelevance \n \nJudgments\n \nDocument Collection\n \nQ1  Q2 Q3\n \n\n \n \nD1\n \nD2\n \nD3\n \nD48\n \n\n \nQueries\n \nD2 +\n \nD1 + \n \nD4  \n-\n \n \nSystem A\n \nSystem B\n \nQuery= \nQ1\n \nTotal # \nRel\n \nDocs = 10\n \nD1 +\n \nD4  \n-\n \nD3  \n-\n \n \nD5 +\n \nD2 +\n \nR\nA\n \nR\nB\n \nPrecision=2/3\n \nPrecision=3/5\n \nRecall=2/10\n \nRecall=3/10\n \n",
    "4": "5\n \nEvaluating a Set of Retrieved Docs: \n \nPrecision and Recall\n \nRelevant Retrieved\n \na\n \nIrrelevant Retrieved\n \nc\n \nIrrelevant Rejected\n \nd\n \nRelevant Rejected\n \nb\n \nRelevant\n \nNot relevant\n \nRetrieved\n \nNot Retrieved\n \nDoc\n \nAction\n \nIdeal results: Precision=Recall=1.0\n \n \nIn \nreality, high recall tends to be \n \nassociated with low \nprecision\n \nSet can be defined by a cutoff (e.g., precision @ 10 docs)  \n \n",
    "5": "Combine Precision and Recall: F\n-\nMeasure\n \n6\n \nP\n: precision\n \nR\n: recall\n \n\n: \nparameter \n(often set to \n1)\n \nWhy not 0.5*P+0.5*R? \n \n",
    "6": "Summary\n \n\nPrecision: are the retrieved results all relevant? \n \n\nRecall: have all the relevant documents been retrieved? \n \n\nF measure combines Precision and Recall\n \n\nTradeoff between Precision and Recall depends on the \n\n \n7\n \n"
}