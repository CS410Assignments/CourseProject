{
    "0": "Syntagmatic Relation Discovery: \n \nMutual Information\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Syntagmatic Relation Discovery: \n \nMutual Information\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "3\n \nMutual Information I(X;Y): Measuring Entropy \nReduction\n \nProperties: \n \n-\nNon\n-\nnegative:  \nI(X;Y)\n\n0\n \n-\nSymmetric:  \nI(X;Y)=I(Y;X\n) \n \n-\n \nI(X;Y)=0  \niff\n \nX & Y are independent\n \nHow much reduction in the entropy of X can we obtain by knowing Y? \n \nMutual Information: \n \nI(X; Y)= H(X) \n\n \nH(X|Y) = H(Y)\n-\nH(Y|X)\n \nWhen we fix X to rank different Ys, I(X;Y) and H(X|Y) give the same order\n \nbut I(X;Y) allows us to compare different (X,Y) pairs.\n \n",
    "3": "4\n \nMutual Information I(X;Y) for \n \nSyntagmatic Relation Mining\n \nMutual Information: \n \nI(X; Y)= H(X) \n\n \nH(X|Y) = H(Y)\n-\nH(Y|X)\n \nI(\nX\neats\n; \nX\nmeats\n) = \nI(\nX\nmeats\n; \nX\neats\n)\n \n \nI(\nX\neats\n; \nX\nthe\n) = \nI(\nX\nthe\n; \nX\neats\n)\n \n \n\neats\n\nother words \nalso tend to occur? \n \nWhich\n \nwords \nhave high mutual information with \n\neats\n\n \n>\n \nI(\nX\neats\n; \nX\neats\n) =H(\nX\neats\n) \n\n \nI(\nX\neats\n; \nX\nw\n) \n \n",
    "4": "Rewriting Mutual Information (MI) \n \nUsing KL\n-\ndivergence\n \n5\n \nThe observed joint \ndistribution of \nX\nW1\n \nand X\nW2\n \nThe expected joint \ndistribution of \nX\nW1\n \nand X\nW2 \n \nif X\nW1\n \nand \nX\nW2 \nwere independent \n \nMI measures the divergence of the actual joint distribution from the expected \ndistribution under the independence assumption. The larger the divergence is, \nthe higher the MI would be. \n \n",
    "5": "Probabilities Involved in Mutual Information\n \n6\n \np(X\nW1\n=1) + p(X\nW1\n=0) = 1\n \nPresence & absence of w1:\n \np(X\nW2\n=1) + p(X\nW2\n=0) = 1\n \nPresence & absence of w2:\n \np(X\nW1\n=1, X\nW2\n=1) + p(X\nW1\n=1\n, \nX\nW2\n=0)+p(X\nW1\n=0, X\nW2\n=1\n) + \np(X\nW1\n=0, X\nW2\n=0) = 1\n \nCo\n-\noccurrences of w1 and w2:\n \nBoth w1 & w2 occur\n \nOnly w1 occurs\n \nOnly w2 occurs\n \nNone of them occurs\n \n",
    "6": "Relations Between Different Probabilities\n \n7\n \np(X\nW1\n=1) + p(X\nW1\n=0) = 1\n \nPresence & absence of w1:\n \np(X\nW2\n=1) + p(X\nW2\n=0) = 1\n \nPresence & absence of w2:\n \np(X\nW1\n=1, X\nW2\n=1) + p(X\nW1\n=1\n, \nX\nW2\n=0)+p(X\nW1\n=0, X\nW2\n=1\n) + \np(X\nW1\n=0, X\nW2\n=0) = 1\n \nCo\n-\noccurrences of w1 and w2:\n \np(\nX\nW1\n=1\n, X\nW2\n=1) + p(\nX\nW1\n=1\n, X\nW2\n=0\n) = p\n(\nX\nW1\n=1\n)\n \np(X\nW1\n=1, \nX\nW2\n=1\n) + \np(X\nW1\n=0\n, \nX\nW2\n=1\n) =\n \np(\nX\nW2\n=1\n)\n  \n \np(\nX\nW1\n=0\n, X\nW2\n=1) + p(\nX\nW1\n=0\n, X\nW2\n=0\n)=\n \np(\nX\nW1\n=0\n) \n \np(X\nW1\n=1, \nX\nW2\n=0\n) \n+ \np(X\nW1\n=0, \nX\nW2\n=0\n) =\n \np(\nX\nW2\n=0\n)  \n \nConstraints: \n \n",
    "7": "Computation of Mutual Information\n \n8\n \np(X\nW1\n=1) + p(X\nW1\n=0) = 1\n \nPresence & absence of w1:\n \np(X\nW2\n=1) + p(X\nW2\n=0) = 1\n \nPresence & absence of w2:\n \np(X\nW1\n=1, X\nW2\n=1) + p(X\nW1\n=1\n, \nX\nW2\n=0)+p(X\nW1\n=0, X\nW2\n=1\n) + \np(X\nW1\n=0, X\nW2\n=0) = 1\n \nCo\n-\noccurrences of w1 and w2:\n \np(\nX\nW1\n=1\n, X\nW2\n=1) + p(\nX\nW1\n=1\n, X\nW2\n=0\n) =  p\n(\nX\nW1\n=1\n)\n \np(X\nW1\n=1, \nX\nW2\n=1\n) + \np(X\nW1\n=0\n, \nX\nW2\n=1\n) =\n \np(\nX\nW2\n=1\n)\n  \n \np(\nX\nW1\n=0\n, X\nW2\n=1) + p(\nX\nW1\n=0\n, X\nW2\n=0\n)=\n \np(\nX\nW1\n=0\n) \n \np(X\nW1\n=1, \nX\nW2\n=0\n) \n+ \np(X\nW1\n=0, \nX\nW2\n=0\n) =\n \np(\nX\nW2\n=0\n)  \n \nWe only need to know \np(X\nW1\n=1\n), \np(X\nW2\n=1\n), and \np(X\nW1\n=1, X\nW2\n=1\n). \n \n",
    "8": "Estimation of Probabilities (Depending on the \nD\nata)\n \n9\n \nW1      W2\n \nSegment_1        1       0\n \nSegment_2        1       1\n \nSegment_3        1       1\n \nSegment_4        0       0\n \nSegment_N\n        \n0        1\n \n\n \nOnly W1 occurred\n \nBoth occurred\n \nNeither occurred\n \n  \nBoth occurred\n \nOnly W2 occurred\n \nCount(w1) = total number segments that contain W1\n \nCount(w2) \n= total number segments that contain \nW2\n \nCount(w1, w2) \n= total number segments that contain \nboth W1 and W2\n \n",
    "9": "Smoothing: Accommodating Zero \nC\nounts\n \n10\n \nW1      W2\n \n\u00bc PseudoSeg_1        0           0\n \n\u00bc PseudoSeg_2        1           0\n \n\u00bc PseudoSeg_3        0           1\n \n\u00bc PseudoSeg_4        1           1\n \nSegment_N\n          \n0          1\n \n\n \nSegment_1           1          0\n \nSmoothing\n: Add pseudo data so that \n \nno event has zero counts \n \n(pretend we observed extra data)\n \nActually observed data\n \n",
    "10": "Summary of Syntagmatic Relation Discovery\n \n\nSyntagmatic relation can be discovered by measuring \ncorrelations between occurrences of two words. \n \n\nThree concepts from Information Theory:\n \n \n\nEntropy H(X): measures the uncertainty of a random variable X \n \n\nConditional entropy H(X|Y): entropy of X given we know Y\n \n\nMutual information I(X;Y): entropy reduction of X (or Y) due to \nknowing Y (or X)\n \n\nMutual information provides a principled way for discovering \nsyntagmatic relations.\n \n11\n \n",
    "11": "Summary of Word Association Mining\n \n\nTwo basic associations: paradigmatic and syntagmatic \n \n\nGenerally applicable to any items in any language (e.g., phrases or \nentities as units)\n \n\nPure statistical approaches are available for discovering both \n(can be combined to perform joint analysis).\n \n\nGenerally applicable to any text with no human effort\n \n\n\nvariations of applications\n \n\nDiscovered associations can support many other applications.\n \n \n12\n \n",
    "12": "Additional Reading\n \n\nChris \nManning and \nHinrich\n \nSch\u00fctze\n, Foundations of \nStatistical Natural Language Processing, MIT Press. \nCambridge, MA: May 1999. (Chapter 5 on collocations) \n \n\nChengxiang Zhai, \nExploiting context to identify lexical \natoms: A statistical view of linguistic context. Proceedings \nof the International and Interdisciplinary Conference on \nModelling and Using Context (CONTEXT\n-\n97), Rio de \nJaneiro, \nBrzil\n, Feb. 4\n-\n6, 1997. \npp. 119\n-\n129\n. \n \n\nShan Jiang and \nChengXiang\n \nZhai, Random \nwalks on \nadjacency graphs for mining lexical relations from big text \ndata\n. Proceedings of IEEE \nBigData\n \nConference 2014, pp. \n549\n-\n554. \n \n \n \n13\n \n"
}