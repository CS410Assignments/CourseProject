{
    "0": "Implementation of Text Retrieval Systems\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Implementation of Text Retrieval Systems\n \n2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n11. Recommendation\n \n3. Text Retrieval Problem\n \n10. Web Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n6. System \n \nImplementation\n \n7. Evaluation\n \n5. Vector Space Model\n \n8. Probabilistic Model \n \n9. Feedback  \n \n",
    "2": "3\n \nTypical TR System Architecture\n \nUser\n \nquery\n \ndocs\n \nresults\n \nQuery Rep\n \nDoc Rep (Index) \n \nScorer\n \nIndexer\n \nTokenizer\n \nIndex\n \njudgments\n \nFeedback\n \nOffline\n \nOnline\n \nOffline & Online\n \n",
    "3": "4\n \nTokenization\n \n\nNormalize lexical units: Words with similar meanings \nshould be mapped to the same indexing term\n \n\nStemming: Mapping all inflectional forms of words to \nthe same root form, e.g.\n \n\ncomputer \n-\n> compute\n \n\ncomputation \n-\n> compute\n \n\ncomputing \n-\n> compute  \n \n\nSome languages (e.g., Chinese) pose challenges in word \nsegmentation\n \n",
    "4": "5\n \nIndexing\n \n\nIndexing = Convert documents to data structures that \nenable fast search (precomputing as much as we can)\n \n\nInverted index is the dominating indexing method for \nsupporting basic search algorithms \n \n\nOther indices (e.g., document index) may be needed for \nfeedback\n \n \n",
    "5": "Doc \nid\n \nFreq\n \n1\n \n1\n \n2\n \n1\n \n3\n \n1\n \n2\n \n1\n \n3\n \n1\n \n3\n \n2\n \n2\n \n1\n \n\n \n\n \n\n \n\n \n6\n \nInverted Index Example\n \ndoc 3\n \nDictionary\n \n(or lexicon)\n \nPostings\n \nTerm\n \n# \ndocs\n \nTotal \nfreq\n \nnews\n \n3\n \n3\n \ncampaign\n \n2\n \n2\n \npresidential\n \n1\n \n2\n \nfood\n \n1\n \n1\n \n\n \n\n \n\n \n\nnews\n \nof \npresidential campaign \n\n \n\npresidential \n\n \n\nnews about \norganic food \ncampaign\n\n \ndoc 1\n \n\nnews about\n \ndoc 2\n \nPosition\n \np1\n \np2\n \np4\n \np3\n \np5\n \np6,p7 \n \np8\n \n",
    "6": "7\n \nInverted Index for Fast Search\n \n\nSingle\n-\nterm query? \n \n\nMulti\n-\nterm Boolean query?\n \n\nM\n\n \n\n\n \n\nMulti\n-\nterm keyword query\n \n\n\n \n\nAggregate term weights  \n \n\nMore efficient than sequentially scanning docs (why?)\n \n",
    "7": "8\n \nEmpirical Distribution of Words\n \n\nThere are stable language\n-\nindependent patterns in how \npeople use natural languages\n \n\nA few words occur very frequently; most occur rarely. \nE.g., in news articles,\n \n\nTop 4 words: 10~15% word occurrences\n \n\nTop 50 words: 35~40% word occurrences\n \n\nThe most frequent word in one corpus may be rare in \nanother\n \n \n \n",
    "8": "9\n \n\n \n\nrank * frequency \n\n \nconstant\n \n \nWord\n \nFreq.\n \nWord Rank (by \nFreq\n)\n \nIntermediate frequency \nwords \n \n \nHighest Frequency\n \nBiggest\n \ndata structure\n \n(stop words)\n \nMany rare words\n \n",
    "9": "10\n \nData Structures for Inverted Index\n \n\nDictionary: modest size\n \n\nNeeds fast random access\n \n\nPreferred to be in memory\n \n\nHash table, B\n-\ntree, \ntrie\n\n \n\nPostings: huge\n \n\nSequential access is expected \n \n\nCan stay on disk\n \n\nMay contain \ndocID\n, term freq., term \npos\n, \netc\n \n\nCompression is desirable\n \n"
}