{
    "0": "System Implementation: Inverted Index Construction\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "System Implementation: Inverted Index Construction\n \n2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n11. Recommendation\n \n3. Text Retrieval Problem\n \n10. Web Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n6. System \n \nImplementation\n \n7. Evaluation\n \n5. Vector Space Model\n \n8. Probabilistic Model \n \n9. Feedback  \n \n",
    "2": "3\n \nConstructing Inverted Index\n \n\nThe main difficulty is to build a huge index with limited \nmemory\n \n\nMemory\n-\nbased methods: not usable for large collections \n \n\nSort\n-\nbased methods: \n \n\nStep 1: Collect local (\ntermID\n, \ndocID\n, \nfreq\n) tuples\n \n\n\n \n\nStep 3: Pair\n-\nwise merge runs\n \n\nStep 4: Output inverted file\n \n",
    "3": "4\n \nSort\n-\nbased Inversion\n \n...\n \nTerm \nLexicon:\n \n \nthe 1\n \ncampaign 2\n \nnews 3\n \na 4\n \n...\n \nDocID\n \nLexicon:\n \n \ndoc1 1\n \ndoc2 2\n \ndoc3 3\n \n...\n \ndoc1\n \ndoc2\n \ndoc300\n \n<1\n,1\n,3>\n \n<2,\n1\n,2>\n \n<3,\n1\n,1>\n \n... \n \n<1,\n2\n,2>\n \n<3,\n2\n,3>\n \n<4,\n2\n,2>\n \n\n \n \n \n \n<1,\n300\n,3>\n \n<3,\n300\n,1>\n \n...\n \nSort by doc\n-\nid\n \nParse & Count\n \n<\n1\n,1,3>\n \n<\n1\n,2,2>\n \n<\n2\n,1,2>\n \n<\n2\n,4,3>\n \n...\n \n<\n1\n,5,3>\n \n<\n1\n,6,2>\n \n\n \n \n \n \n<\n1\n,299,3>\n \n<\n1\n,300,1>\n \n...\n \nSort by term\n-\nid\n \n\nSort\n \n<\n1\n,1,3>\n \n<\n1\n,2,2>\n \n<\n1\n,5,2>\n \n<\n1\n,6,3>\n \n...\n \n<\n1\n,300,3>\n \n<\n2\n,1,2>\n \n\n \n \n \n \n<\n5000\n,299,1>\n \n<\n5000\n,300,1>\n \n...\n \nMerge \nSort\n \nAll info about term \n1\n \n",
    "4": "5\n \nInverted Index Compression\n \n\nIn general, leverage skewed distribution of values and use \nvariable\n-\nlength encoding\n \n\nTF compression\n \n\nSmall numbers tend to occur far more frequently than large \nnumbers (why?) \n \n\nF\newer \nbits for small (high frequency) \nintegers at the cost of more \nbits for large integers \n \n\nDoc ID compression\n \n\n\n-\n\n-\nd1, d3\n-\n\n \n\nFeasible due to sequential access \n \n\nMethods: Binary code, unary code, \n\n-\ncode, \n\n-\n\n \n",
    "5": "6\n \nInteger Compression Methods\n \n\nBinary: equal\n-\nlength coding\n \n\nUnary: x\n\n1\n \nis coded as x\n-\n1 one bits followed by 0, e.g., \n3=> 110; 5=>11110\n \n\n\n-\ncode: x=> unary code for 1+\n\nlog x\n\n \nfollowed by  uniform \ncode for x\n-\n2 \n\nlog x\n\n \nin \n\nlog x\n\n  \nbits, e.g., 3=>101, 5=>11001\n \n\n\n-\ncode: same as \n\n-\ncode ,but replace the unary prefix with \n\n-\ncode. E.g., 3=>1001, 5=>10101\n \n",
    "6": "Uncompress\n \nInverted Index\n \n\nDecoding of encoded integers\n \n\n\n \n\n\n-\ndecoding\n \n\nfirst decode the unary part; let value be  k+1\n \n\nread k more bits decode them as binary code; let value be r\n \n\nthe value of the encoded number is \n2\nk\n+r\n \n\nDecode doc IDs encoded using d\n-\ngap\n \n\n\n \n\nDecode x1 to obtain doc ID1; then decode x2 and add the \nrecovered value to the doc ID1 just obtained\n \n\n\nprevious doc ID. \n \n \n \n7\n \n"
}