{
    "0": "Probabilistic Topic Models: \n \nMixture of Unigram Language Models \n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Probabilistic Topic Models: Mixture of Unigram LMs\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "Factoring \no\nut Background Words\n \n3\n \nthe 0.031\n \na 0.018\n \n\n \ntext  0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering 0.005\n \ncomputer 0.0009\n \n\n \nfood 0.000001\n \n\n \nText mining\n \npaper\n \nd\n \np(w|\n \n\n)\n \nHow can we get rid of\n \nthese common words?\n \n",
    "3": "Generate d Using \nT\nwo \nW\nord \nD\nistributions\n \n4\n \ntext  \n0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering \n0.005\n \n\n \nthe\n \n0.000001\n \nText mining\n \npaper\n \nd\n \nTopic: \n\nd\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  \n0.000006\n \n\n \nBackground\n \n(topic) \n\nB\n \nP(w|\n \n\nd\n)\n \np(w|\n \n\nB\n)\n \nP(\n\nd\n)=0.5\n \nP(\n\nB\n)=0.5\n \nTopic \n \nChoice\n \np(\n\nd \n)+(\n\nB\n)=1\n \n",
    "4": "\n \n5\n \ntext  \n0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering \n0.005\n \n\n \nthe\n \n0.000001\n \nd\n \nTopic: \n\nd\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  \n0.000006\n \n\n \nBackground\n \n(topic) \n\nB\n \nP(w|\n \n\nd\n)\n \np(w|\n \n\nB\n)\n \nP(\n\nd\n)=0.5\n \nP(\n\nB\n)=0.5\n \nTopic \n \nChoice\n \np(\n\nd \n)+(\n\nB\n)=1\n \n\ntext\n\n \n\n \n\np(\n\nd\n\n\nd\n) \n+ \np\n(\n\nB\n\n \n\nB\n)\n \n               \n= \n0.5*0.000001\n+0.5*0.03\n \n\ntext\n\np(\n\nd\n\n\nd\n) \n+ \np(\n\nB\n\n\nB\n)\n \n               \n= \n0.5*0.04\n+0.5*0.000006\n \n",
    "5": "The Idea of a Mixture \nM\nodel  \n \n6\n \ntext  \n0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering \n0.005\n \n\n \nthe\n \n0.000001\n \n\nd\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  0.000006\n \n\nB\n \nP(\n\nd\n)=0.5\n \nP(\n\nB\n)=0.5\n \nTopic \n \nChoice\n \np(\n\nd \n)+(\n\nB\n)=1\n \n\ntext\n\n \n\n \nw\n \nMixture Model\n \n",
    "6": "As a Generative \nM\n\n \n7\n \nFormally defines the following generative model:   \n \np\n(w)=\np(\n\nd\n)p(w|\n\nd\n) \n+ p(\n\nB \n)p(w| \n\nB\n)\n \nw\n \nWhat if p(\n\nd\n \n)=1 or p(\n\nB \n)=1?\n \n\n \ntwo topics \n+ \ntopic coverage\n \n",
    "7": "Mixture of Two Unigram Language Models\n \n\nData\n: Document d \n \n\nMixture \nModel\n:  \nparameters\n \n\n=({p(w|\n\nd \n)}, {\np(w\n|\n\nB \n)}, p\n(\n\nB\n), p\n(\n\nd \n))\n \n\nTwo unigram LMs: \n\nd \n \n(the topic of d); \n\nB\n \n(background topic)\n \n\nMixing weight (topic choice):  \np(\n\nd \n)+p(\n\nB\n)=\n1\n \n\nLikelihood\n \nfunction: \n \n \n \n \n\nML\n \nEstimate:\n \n \n \n8\n \nSubject to\n \n"
}