{
    "0": "Evaluation of TR Systems: Evaluating a Ranked List\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Evaluation of TR Systems: Evaluating a Ranked List\n \n2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n11. Recommendation\n \n3. Text Retrieval Problem\n \n10. Web Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n7. Evaluation \n \n6. System \n \nImplementation\n \n5. Vector Space Model\n \n8. Probabilistic Model \n \n9. Feedback  \n \n",
    "2": "Mean Average Precision (MAP)\n \n\nAverage Precision: \n \n\nT\nhe average of precision at every cutoff where a new relevant \ndocument is retrieved \n \n\nNormalizer = the total # of relevant docs in collection\n \n\nSensitive \n \nto the \nrank of each relevant document\n \n\nMean Average Precisions (\nMAP)\n \n\nMAP \n= arithmetic mean \nof average \nprecision over a set of \nqueries\n \n\ngMAP\n \n= geometric mean \nof average \nprecision over a set of \nqueries \n \n\nWhich is better: MAP or \ngMAP\n?\n \n \n6\n \n",
    "3": "Special Case: Mean Reciprocal Rank\n \n\n\ncollection (e.g., known item search)\n \n\nAverage Precision = Reciprocal Rank = 1/r, where r is the rank \nposition of the single relevant doc\n \n\nMean Average Precision \n\n  \nMean Reciprocal Rank\n \n\nWhy not simply use r? \n \n \n7\n \n",
    "4": "Summary\n \n\nPrecision\n-\nRecall curve characterizes the overall accuracy \nof a ranked list\n \n\nThe \nactual\n \nutility of a ranked list depends on how many \ntop\n-\nranked results a user would examine\n \n\nAverage Precision is the standard measure for comparing \ntwo ranking methods\n \n\nCombines precision and recall\n \n\nSensitive to the rank of \nevery\n \nrelevant document\n \n \n8\n \nWhat if we have multiple levels of relevance judgments? \n \n"
}