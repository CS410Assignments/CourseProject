{
    "0": "Text Categorization: Methods\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Overview\n \n\nWhat is text categorization?\n \n\nWhy text categorization?\n \n\nHow to do text categorization?\n \n\nGenerative probabilistic models\n \n\nDiscriminative approaches\n \n\nHow to evaluate categorization results? \n \n \n2\n \n",
    "2": "Categorization Methods: Manual\n \n\nD\netermine the category based on rules that\n \nare carefully designed \nto reflect the domain knowledge about the categorization \nproblem\n \n\nWorks well when\n \n\nThe categories are very well defined\n \n\nCategories are easily distinguished based \non surface \nfeatures in text \n(e.g., special vocabulary is known to only occur in a particular category) \n \n\nSufficient domain knowledge is available to suggest many effective rules \n \n\nProblems\n \n\nLabor intensive \n\n \n\n \n\n\n\n \nnot \nrobust \n \n \n\nBoth problems can be solved/alleviated by using machine learning \n \n \n3\n \n",
    "3": "\n \n\nUse \nhuman experts \nto \n \n\nAnnotate \ndata sets with \ncategory \nlabels \n\n \nTraining data\n \n\nP\nrovide \na set of \nfeatures\n \nto represent each text object that can potentially \n\n \n\nUse \nmachine learning \n\nthe training data \n \n\nFigure out \nwhich features are most useful \nfor separating different \ncategories\n \n\nO\nptimally combine the features \nto \nminimize the errors \nof categorization \non the training data \n \n\nThe trained classifier can then be applied to a new text object to predict \nthe most likely category (that a human expert would assign to it)\n \n4\n \n",
    "4": "Machine Learning for Text Categorization\n \n\nGeneral setup\n: Learn a classifier  f: X\n\nY \n \n\nInput: X = all text objects; Output: Y = all categories \n \n\nLearn a classifier function, f: X\n\nY, such that f(x)=y \n\nY gives the correct \ncategory for \nx\n\nX\n \n\n \n\nAll methods \n \n\nR\nely on discriminative features of text objects to distinguish categories\n \n\nC\nombine multiple features in a weighted manner\n \n\nA\ndjust weights on features to minimize errors on the training data\n \n\nDifferent methods \ntend to vary in  \n \n\nTheir way of measuring the errors on the training data (may optimize a \ndifferent objective/loss/cost function)\n \n\nTheir way of combining features (e.g., linear vs. non\n-\nlinear)\n \n5\n \n",
    "5": "Generative vs. Discriminative Classifiers\n \n\nGenerative\n \nclassifiers (learn \n\n \nin each \ncategory\n)\n \n\nAttempt to model p(X,Y) = p(Y)p(X|Y) and compute p(Y|X) based on p(X|Y) \nand p(Y) by using Bayes Rule\n \n\nObjective function is likelihood, thus indirectly measuring training errors \n \n\nE.g., Na\u00efve Bayes\n \n\nDiscriminative \nclassifiers (learn \nwhat features separate categories\n)\n \n\nAttempt to model p(Y|X) directly \n \n\nObjective function directly measures errors of categorization on training \ndata\n \n\nE.g., \nLogistic Regression, Support \nVector Machine (SVM), k\n-\nNearest \nNeighbors (\nkNN\n)\n \n \n \n \n \n \n \n6\n \n"
}