{
    "0": "1\n \nText Retrieval Problem\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n",
    "1": "Course Schedule\n \n2\n \nBig Text Data\n \nSmall Relevant Data\n \n  \nSearch Engine\n \nRecommender \n \nSystem\n \n2. Text Access\n \n11. \nRecommendation\n \n10. \nWeb Search\n \nUser\n \n1. Natural Language Content Analysis\n \n4. Text Retrieval Methods\n \n7\n. \nEvaluation \n \n6. System \n \nImplementation\n \n5\n. Vector Space Model\n \n8\n. Probabilistic Model \n \n9. Feedback  \n \n3. Text Retrieval Problem\n \n",
    "2": "Overview\n \n\nWhat is Text Retrieval?\n \n\nText Retrieval vs. Database Retrieval\n \n\nDocument Selection vs. Document Ranking\n \n3\n \n",
    "3": "What Is Text Retrieval (TR)?\n \n\nC\nollection of text documents exists\n \n\nUser gives a query to express the information need\n \n\nS\nearch engine\n \nsystem returns relevant documents to \nusers\n \n\nO\n\nactually much broader\n \n\n\n \n4\n \n",
    "4": "TR vs. Database Retrieval\n \n\nInformation\n \n\nUnstructured/free text vs. structured data\n \n\nAmbiguous vs. well\n-\ndefined semantics\n \n\nQuery \n \n\nAmbiguous vs. well\n-\ndefined semantics\n \n\nIncomplete vs. complete specification\n \n\nAnswers\n \n\nRelevant documents vs. matched records\n \n\nTR is an empirically defined problem\n \n\n\n \n\nMust rely on \nempirical evaluation \ninvolving users!\n \n5\n \n",
    "5": "Formal Formulation of TR\n \n\nVocabulary\n: V={w\n1\n, w\n2\n\nw\nN\n} of language\n \n\nQuery\n: q = q\n1\n\nq\nm\n,\n \nwhere q\ni \n\n \nV\n \n\nDocument\n: d\ni\n \n= d\ni1\n\nd\nim\ni\n,\n \nwhere \nd\nij\n \n\n \nV\n \n\nCollection\n: C= {d\n1\n\nd\nM\n}\n \n\nSet of relevant documents\n: R(q) \n\n \nC\n \n\nGenerally unknown and user\n-\ndependent\n \n\n\n \n\nTask\n \n\n \n6\n \n",
    "6": "How to \nC\n\n \n\nStrategy 1: Document selection\n \n\n\nd\n\nC|f\n(\nd,q\n)=1}, where f(\nd,q\n) \n\n{0,1} is an indicator function or \nbinary classifier\n \n\nSystem must decide if a doc is relevant or not (\nabsolute relevance\n)\n \n\nStrategy 2: Document ranking\n \n\n\nd\n\nC|f\n(\nd,q\n)>\n\n}, where f(\nd,q\n) \n\n \nis a relevance measure \nfunction; \n\n \nis a cutoff determined by the user\n \n\nSystem \nonly needs to\n \ndecide if one doc is more likely relevant than \nanother (\nrelative relevance\n)\n \n \n7\n \n",
    "7": "8\n \nDocument Selection vs. Ranking\n \n+\n \n+\n \n+\n \n+\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n+\n \n-\n \n-\n \nDoc Selection\n \nf(d,q)=?\n \n+\n \n+\n \n+\n \n+\n \n-\n \n-\n \n+\n \n-\n \n+\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \n-\n \nDoc Ranking\n \nf(d,q)=?\n \n1\n \n0\n \n0.98 d\n1\n \n+\n \n0.95 d\n2\n \n+\n \n0.83 d\n3\n \n-\n \n0.80 d\n4\n \n+\n \n0.76 d\n5\n \n-\n \n0.56 d\n6\n \n-\n \n0.34 d\n7\n \n-\n \n0.21 d\n8\n \n+\n \n\n \n\n \nTrue R(q)\n \n",
    "8": "Problems of Document Selection\n \n\nThe classifier is unlikely accurate\n \n\n\n-\n\n\n \nno relevant documents \nto return\n \n\n\n-\n\n\n \nover delivery\n \n\nH\nard to find the right position between these two extremes\n \n\nEven if it is accurate, all relevant documents are not \nequally relevant (relevance is a matter of degree!)     \n \n\nPrioritization \nis \nneeded\n \n\nThus, ranking is generally preferred \n \n9\n \n",
    "9": "Theoretical Justification for Ranking\n \n\nProbability Ranking Principle \n[Robertson 77]: Returning \na ranked \nlist of documents in descending order of \nprobability that \na \ndocument is relevant to the query is the optimal strategy under \nthe following two \nassumptions:\n \n\nThe utility of a document (to a user) is \nindependent\n \nof the \nutility of any other document \n \n\nA user would browse the results \nsequentially\n \n\nDo these two assumptions hold? \n \n \n10\n \n",
    "10": "Summary\n \n\nText retrieval is an empirically defined problem              \n \n\nWhich algorithm is better must be judged by users\n              \n \n\nDocument ranking is generally preferred to \n \n\nHelp users prioritize examination of search results \n \n\nBypass the difficulty in determining absolute relevance (users \nhelp decide the cutoff on the ranked list)\n \n\nMain challenge: design an effective ranking \nfunction  \nf(\nq,d\n) =?  \n \n \n11\n \n",
    "11": "Additional Readings\n \n\nS.E. Robertson, The probability ranking principle in IR. \nJournal of Documentation\n \n33\n, \n294\n-\n304, 1977\n \n\nC. J. van \nRijsbergen\n, Information Retrieval,  2\nnd\n \nEdition, \nButterworth\n-\nHeinemann, Newton, MA, USA, \n1979\n \n\nA must\n-\nread for anyone doing research in information \nretrieval. Chapter 6 has an in\n-\ndepth discussion of PRP.\n \n \n12\n \n"
}