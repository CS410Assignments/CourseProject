{
    "0": "Opinion Mining and Sentiment Analysis:\n \nSentiment Classification  \n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Sentiment Classification\n \n2\n \nText Data\n \nOpinion Holder\n \nOpinion Target\n \nOpinion Content\n \nOpinion Context\n \nAll Known\n \nOpinion\n \nSentiment\n \n?\n \n",
    "2": "Sentiment Classification: Task Definition\n \n3\n \n\nInput: An opinionated text object\n \n\nOutput: A sentiment tag/label\n \n\nPolarity analysis: e.g., categories = {positive, negative, neutral}, or \ncategories ={5, 4, 3, 2, 1}\n \n\nEmotion analysis (beyond polarity): e.g., categories ={happy, sad, \nfearful, angry, surprised\n, disgusted}\n \n\nA special case of text categorization! \n\n \nAny text categorization \nmethod can be used to do sentiment classification\n \n \n\nFurther improvement comes from\n \n\nMore sophisticated features appropriate for sentiment tagging\n \n\nConsideration of the order of the categories (e.g., ordinal regression)  \n \n",
    "3": "Commonly Used Text Features\n \n\nCharacter n\n-\n\n \n\nGeneral and robust to spelling/recognition errors, but less \ndiscriminative than words\n \n\nWord n\n-\n\n \n\nUnigrams are often very effective, but not for sentiment \n\n \n\nLong n\n-\ngrams are discriminative, but may cause overfitting\n \n\nPOS tag n\n-\ngrams: mixed n\n-\ngram with words and POS tags\n \n\n\n \n4\n \n",
    "4": "Commonly Used Text Features (cont.)\n \n\nWord classes \n \n\nSyntactic (= POS tags)\n \n\nSemantic Concept: e.g., thesaurus/ontology, recognized entities \n \n\nEmpirical word clusters (e.g., cluster of paradigmatically or \nsyntagmatically\n \nrelated words)\n \n\nFrequent patterns in text (e.g., frequent word set; collocations) \n \n\nMore specific/discriminative than words\n \n\nMay generalize better than pure n\n-\ngrams\n \n\nParse tree\n-\nbased (e.g., frequent subtrees, paths)\n \n\nEven more discriminative, but need to avoid overfitting\n \n\nPattern discovery algorithms are very useful for feature construction\n \n5\n \n",
    "5": "\n \n\nAdv\n \nAdj\n\n \nVerb\n \nNP\n \n\n \n\n \nA \n  \ndog   is   chasing   a   boy   on   the   playground\n \n6\n \nA \n  \ndog   is   chasing   a   boy   on   the   playground\n \nDet\n \nNoun\n \nAux\n \nVerb\n \nDet\n \nNoun\n \nPrep\n \nDet\n \nNoun\n \nNoun Phrase\n \nComplex Verb\n \nNoun Phrase\n \nNoun Phrase\n \nPrep Phrase\n \nVerb Phrase\n \nVerb Phrase\n \nSentence\n \nA dog\n \nAnimal\n \nA boy\n \nPerson\n \nt\nhe playground\n \nLocation\n \nCHASE\n \nON\n \nDog(d1\n). Boy(b1). Playground(p1). Chasing(d1,b1,p1\n).\n \nSpeech Act = REQUEST\n \nNLP Enriches Text Representation with \n \nComplex Features\n \n",
    "6": "Feature Construction for Text Categorization\n \n\nFeature design affects categorization accuracy significantly \n \n\nA combination of machine learning, error analysis, and domain \nknowledge is most effective\n \n\nDomain knowledge \n\n \nseed features, feature space \n \n\nMachine learning \n\n \nfeature selection, feature learning \n \n\nError analysis \n\n \nfeature validation \n \n\nNLP enriches text representation \n\n \nenriches feature space \n(more likely overfitting!)\n \n\nOptimizing the tradeoff between \nexhaustivity\n \nand \nspecificity\n \nis \na major goal\n \n7\n \nhigh coverage (frequent) \n \ndiscriminative (infrequent) \n \n"
}