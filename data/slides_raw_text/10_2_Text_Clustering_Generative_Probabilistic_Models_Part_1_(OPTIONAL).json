{
    "0": "Text Clustering: Generative Probabilistic \nModels (Part 1)\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Text Clustering: Generative Probabilistic Models \n(Part 1)\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "Overview\n \n\nWhat is text clustering?\n \n\nWhy text clustering?\n \n\nHow to do text clustering?\n \n\nGenerative probabilistic models\n \n\nSimilarity\n-\nbased approaches\n \n\nHow to evaluate clustering results? \n \n \n3\n \n",
    "3": "Topic Mining Revisited\n \n4\n \n\n \nDoc 2\n \nDoc N\n \n\n \nDoc 1\n \n\n1\n \n\n2\n \n\nk\n \n\n11\n \n\n12\n \n\n1k\n \n\n21\n=0%\n \n\n2\n2\n \n\n2\nk\n \n\nN1\n=0%\n \n \n\nN\n2\n \n\nN\nk\n \n30%\n \n1\n2\n%\n \n8\n%\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \n\n \ntravel  0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \n\n \nINPUT:  C, k, V\n \nOUTPUT: { \n\n1\n\n\nk\n \n}, \n{ \n\ni1\n\n\nik\n \n}\n \nText Data\n \n",
    "4": "One \nT\nopic(=cluster)\n \nP\ner \nD\nocument\n \n5\n \n\n \nDoc 2\n \nDoc N\n \n\n \nDoc 1\n \n\n1\n \n\n2\n \n\nk\n \n\n11\n=\n100%\n \n \n\n12\n=0\n \n\n1k\n=0\n \n \n\n21\n=0%\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \n\n \ntravel  0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \n\n \nINPUT:  C, k, V\n \nOUTPUT: { \n\n1\n\n\nk\n \n},\n \nText Data\n \n\n2\n2\n=100%\n \n\n1k\n=0\n \n \n\nN\n2\n=0\n \n\nN\nk\n=0\n \n \n\nN\n1\n=\n100%\n \n \n{ \nc\n1\n\nc\nN\n \n} c\ni \n\n[1,k]\n \n",
    "5": "Mining One Topic Revisited\n \n6\n \nDoc d\n \n\n \n100%\n \nINPUT:  C={d}, V\n \nOUTPUT: { \n\n}\n \nText Data\n \ntext  ?\n \nmining   ?\n \nassociation ?\n \ndatabase   ? \n \n\n \nquery      ?   \n \n\n \nP(w|\n\n)\n \n(\n1 Doc, 1 Topic) \n \n \n                  \n\n \n(N Docs, N Topics)                     k<N\n \n \n                                           \n\n \n(N Docs, k \nShared\n \nTopics)=Clustering!\n \n \n",
    "6": "What Generative \nM\nodel \nC\nan \nD\no \nC\nlustering?\n \n7\n \n\n \nDoc 2\n \nDoc N\n \n\n \nDoc 1\n \n\n1\n \n\n2\n \n\nk\n \n\n11\n=\n100%\n \n \n\n12\n=0\n \n\n1k\n=0\n \n \n\n21\n=0%\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \n\n \ntravel  0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \n\n \nINPUT:  C, k, V\n \nOUTPUT: { \n\n1\n\n\nk\n \n},\n \nText Data\n \n\n2\n2\n=100%\n \n\n1k\n=0\n \n \n\nN\n2\n=0\n \n\nN\nk\n=0\n \n \n\nN\n1\n=\n100%\n \n \n{ \nc\n1\n\nc\nN\n \n} c\ni \n\n[1,k]\n \nHow can we force every document to be generated \n \nusing \none topic \n(instead of k topics)? \n \n",
    "7": "Generative Topic Model Revisited\n \n8\n \ntext  \n0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering \n0.005\n \n\n \nthe\n \n0.000001\n \n\n1\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  0.000006\n \n\n2\n \nP(\n\n1\n)=0.5\n \nP(\n\n2\n)=0.5\n \nTopic \n \nChoice\n \np(\n\n1\n \n)+p(\n\n2\n)=1\n \n\ntext\n\n \n\n \nw\n \nd\n \n\n \n",
    "8": "Mixture Model for Document Clustering\n \n9\n \ntext  \n0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering \n0.005\n \n\n \nthe\n \n0.000001\n \n\n1\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  0.000006\n \n\n2\n \nP(w|\n \n\n1\n)\n \np(w|\n \n\n2\n)\n \nP(\n\n1\n)=0.5\n \nP(\n\n2\n)=0.5\n \nTopic \n \nChoice\n \np(\n\n1\n \n)+p(\n\n2\n)=1\n \nL\n \nd\n \nd\n \nd=x\n1\n \nx\n2 \n\nx\nL\n \nL\n \nDifference from\n \nt\nopic model?\n \nWhat if \nP(\n\n1\n)=1 \n \nor \nP\n(\n\n2\n)=1?\n \n",
    "9": "10\n \nd=x\n1\n \nx\n2 \n\nx\nL\n \nHow is this different from a topic model? \n \nLikelihood Function: p(d)=? \n \n"
}