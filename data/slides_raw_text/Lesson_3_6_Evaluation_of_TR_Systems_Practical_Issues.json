{
    "0": "Evaluation of TR Systems: Practical Issues\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "2\n \n",
    "2": "Challenges in Creating a Test Collection \n \n3\n \nQ1  D1  +\n \nQ1  D2  +\n \nQ1  D3 \n\n \nQ1  D4 \n\n \nQ1  D5 +\n \n\n \nQ2  D1 \n\n \nQ2  D2 +\n \nQ2  D3 +\n \nQ2  D4 \n\n \n\n \nQ50 D1 \n\n \nQ50 D2 \n\n \nQ50 D3 +\n \n\n \nRelevance \n \nJudgments\n \nQ1  Q2 Q3\n \n\n \n \nD1\n \nD2\n \nD3\n \nD48\n \n\n \nJudgments: \n \ncompleteness vs.\n \n \nminimum human work\n \nQueries\n: representative & many\n \nDocs:\n \nrepresentative & many\n \nExistence of \n \nrelevant docs\n \nMeasures\n: capture the \n \nperceived utility by users\n \n",
    "3": "Statistical Significance Tests\n \n\n\nsimply result from the particular queries you chose?\n \nSystem A\n \n0.20\n \n0.21\n \n0.22\n \n0.19\n \n0.17\n \n0.20\n \n0.21\n \nSystem B\n \n0.40\n \n0.41\n \n0.42\n \n0.39\n \n0.37\n \n0.40\n \n0.41\n \nExperiment 1\n \nQuery\n \n1\n \n2\n \n3\n \n4\n \n5\n \n6\n \n7\n \nAverage\n \n0.20\n \n0.40\n \nSystem A\n \n0.02\n \n0.39\n \n0.16\n \n0.58\n \n0.04\n \n0.09\n \n0.12\n \nSystem B\n \n0.76\n \n0.07\n \n0.37\n \n0.21\n \n0.02\n \n0.91\n \n0.46\n \nExperiment 2\n \nQuery\n \n1\n \n2\n \n3\n \n4\n \n5\n \n6\n \n7\n \nAverage\n \n0.20\n \n0.40\n \nSlide from Doug \nOard\n \n& Philip \nResnik\n \n4\n \n",
    "4": "Statistical Significance Testing\n \nSystem A\n \n0.02\n \n0.39\n \n0.16\n \n0.58\n \n0.04\n \n0.09\n \n0.12\n \nSystem B\n \n0.76\n \n0.07\n \n0.37\n \n0.21\n \n0.02\n \n0.91\n \n0.46\n \nQuery\n \n1\n \n2\n \n3\n \n4\n \n5\n \n6\n \n7\n \nAverage\n \n0.20\n \n0.40\n \nSign Test\n \n+\n \n-\n \n+\n \n-\n \n-\n \n+\n \n+\n \np\n=1.0\n \nWilcoxon\n \n+0.74\n \n-\n \n0.32\n \n+0.21\n \n-\n \n0.37\n \n-\n \n0.02\n \n+0.82\n \n+\n0.34\n \np\n=0.9375\n \n0\n \n95% of outcomes\n \nTry some out at: http://www.fon.hum.uva.nl/Service/CGI\n-\nInline/HTML/Statistics.html\n \n5\n \nSlide from Doug \nOard\n \n& Philip \nResnik\n \n",
    "5": "Pooling: Avoid Judging all Documents\n \n\n\nwhich subset should we judge? \n \n\nPooling strategy\n \n\nChoose a diverse set of ranking methods (TR systems)\n \n\nHave each to return top\n-\nK documents\n \n\nCombine all the top\n-\nK sets to form a pool for human assessors to \njudge\n \n\nOther (unjudged) documents are usually assumed to be non\n-\nrelevant \n\n \n\nO\nkay for comparing systems that contributed to the pool, but \nproblematic for evaluating new systems\n \n6\n \n",
    "6": "Summary of TR Evaluation \n \n\nExtremely important!\n \n\nTR is an empirically defined problem\n \n\nInappropriate experiment design misguides research and applications \n \n\nMake sure to get it right for your research or application\n \n\nCranfield\n \nevaluation methodology is the main paradigm\n \n\nMAP and \nnDCG\n: appropriate for comparing ranking algorithms\n \n\n\n \n\nNot covered\n \n\nA\n-\nB Test  [Sanderson 10]\n \n\nUser studies  [Kelly 09]\n \n7\n \n",
    "7": "Additional Readings\n \n\nDonna \nHarman, Information \nRetrieval Evaluation. Synthesis \nLectures on Information Concepts, Retrieval, and Services, Morgan \n& Claypool Publishers \n2011\n \n \n\nMark Sanderson, Test \nCollection Based Evaluation of Information \nRetrieval Systems. Foundations and Trends in Information Retrieval \n4(4): 247\n-\n375 (2010\n)\n \n \n\nDiane Kelly,  Methods \nfor Evaluating Interactive Information \nRetrieval Systems with Users. Foundations and Trends in \nInformation Retrieval 3(1\n-\n2): 1\n-\n224 (2009\n)\n \n \n \n8\n \n"
}