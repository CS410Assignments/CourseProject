{
    "0": "Paradigmatic Relation Discovery: Part 1\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Paradigmatic Relation Discovery\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining & analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "\n \n3\n \n \ncat: \n \nMy  \n___\n \neats  fish on Saturday\n \nHis  \n___\n \neats  turkey on Tuesday\n \n\n \n\n \n\n \n\n \n    \n\n \n \n          \n\n \n\n \nContext may contain adjacent or non\n-\nadjacent words\n \n",
    "3": "Measuring Context Similarity\n \n4\n \n\n \nSim(\nLeft1\n\nLeft1\n\n \n+ Sim(\nRight1\n\nRight1\n\n \n     \n\n \n+ Sim(\nWindow8\n\nWindow8\n\n \n \nHigh \nsim(word1, word2) \n \n \n          \n\n \nword1 and word2 are \nparadigmatically related\n \n \n",
    "4": "5\n \nBag of Words  \n\n \n \nVector Space Model (VSM)\n \nword\n1\n \nword\n2\n \nword\nN\n \nd1=(\nx\n1\n\nN\n)\n \n\ncat\n\n \nd2=(\ny\n1\n\ny\nN\n)\n \n\ndog\n\n \n \n\n \n \n\n \nN: vocabulary size\n \n",
    "5": "6\n \nVSM for Paradigmatic Relation Mining\n \nd2=(\ny\n1\n\ny\nN\n)\n \nd1=(\nx\n1\n\nx\nN\n)\n \nword\n1\n \nword\n2\n \nword\nN\n \n2\n. Sim(\nd1\n,\nd2\n)=?\n \n \nx\ni\n \n=?\n \ny\nj\n \n=?\n \n \n1. How to compute each vector?\n \nMany approaches are possible \n \n(most developed originally for text retrieval).\n \n",
    "6": "Expected Overlap of Words in Context (EOWC)\n \nSim(\nd1\n,\nd2\n)=\nd1\n.\nd2\n= \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \nd1=(\nx\n1\n\nN\n)\n \nd2=(\ny\n1\n\ny\nN\n)\n \nx\ni\n \n=c(\nw\ni\n \n,\nd1)/|d1|\n \ny\ni\n \n= \nc(\nw\ni\n \n,\nd2)/|d2|\n \n7\n \nProbability that two randomly picked words from d1 and d2, \nrespectively, are identical. \n \nTotal counts of \nwords in d1\n \nCount of word \nwi\n \nin d1\n \nProbability that a randomly \npicked word from d1 is \nwi\n \n",
    "7": "Would EOWC Work Well? \n \n\nIntuitively, it makes sense: The more overlap the two \ncontext documents have, the higher the similarity would be.\n \n\nHowever: \n \n\nI\nt favors matching one frequent term very well over matching \nmore distinct terms.\n \n\n\n\n \n8\n \n",
    "8": "Expected Overlap of Words in Context (EOWC)\n \nSim(\nd1\n,\nd2\n)=\nd1\n.\nd2\n= \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \nd1=(\nx\n1\n\nN\n)\n \nd2=(\ny\n1\n\ny\nN\n)\n \nx\ni\n \n=c(\nw\ni\n \n,\nd1)/|d1|\n \ny\ni\n \n= \nc(\nw\ni\n \n,\nd2)/|d2|\n \n2\n \nProbability that two randomly picked words from d1 and d2, \nrespectively, are identical. \n \nTotal counts of \nwords in d1\n \nCount of word \nwi\n \nin d1\n \nProbability that a randomly \npicked word from d1 is \nwi\n \n",
    "9": "Improving EOWC with Retrieval Heuristics \n \n\nI\nt favors matching one frequent term very well over \nmatching more distinct terms.\n \n \n \n\nI\n\n\n \n3\n \n\n \nS\nublinear\n \ntransformation of Term Frequency (TF)\n \n\n \nReward matching a rare \nword:  \nIDF term \nweighting \n \n",
    "10": "TF Transformation: c(\nw,d\n)\n\nTF(\nw,d\n)\n  \n \n4\n \nTerm Frequency Weight\n \ny=TF(\nw,d\n)\n \n \nx=c(\nw,d\n)\n \n\n \ny= x\n \n1\n \n2\n \n0/1 bit \nvector \n \n(ignore counts)\n \ny= log(1+x)\n \n",
    "11": "TF Transformation: BM25 Transformation  \n \n5\n \nTerm Frequency Weight\n \ny=TF(\nw,d\n)\n \n \nx=c(\nw,d\n)\n \n\n \n1\n \n2\n \ny=\n\n\n\n\n\n\n\n \nk+1\n \nk=0\n \nVery large k\n \n",
    "12": "IDF Weighting: Penalizing Popular Terms\n \nIDF(W)\n \nk (doc \nfreq\n)\n \nIDF(W) = log[(M+1)/k]\n \nM \n \n1\n \nlog(M+1)\n \ntotal number of docs in collection\n \ntotal number of docs containing W\n \n(Doc Frequency)\n \n6\n \n",
    "13": "Adapting BM25 Retrieval Model for \n \nParadigmatic Relation Mining\n \nSim(\nd1\n,\nd2\n)=\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \nd1=(\nx\n1\n\nN\n)\n \nd2=(\ny\n1\n\ny\nN\n)\n \ny\ni\n \nis defined similarly\n \n7\n \n",
    "14": "BM25 can also Discover Syntagmatic Relations \n \nd1=(\nx\n1\n\nN\n)\n \n8\n \nIDF\n-\nweighted d1=(\nx\n1\n*IDF(w\n1\n\nx\nN\n*IDF(w\nN\n))\n \nThe highly weighted terms in the context vector of word w \n \nare likely \nsyntagmatically\n \nrelated to w.\n \n",
    "15": "Summary\n \n\nMain idea for discovering paradigmatic relations:\n \n\nCollecting the context of a candidate word to form a pseudo \ndocument (bag of words)\n \n\nComputing similarity of the corresponding context documents of two \ncandidate words\n \n\nHighly similar word pairs can be assumed to have paradigmatic \nrelations\n \n\nMany different ways to implement this general idea\n \n\nText retrieval models can be easily adapted for computing \nsimilarity of two context documents\n \n\nBM25 + IDF weighting represents the state of the art\n \n\n\n \n9\n \n"
}