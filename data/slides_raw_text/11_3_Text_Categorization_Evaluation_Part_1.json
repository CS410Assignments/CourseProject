{
    "0": "Text Categorization\n: Evaluation (Part 1)\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Overview\n \n\nWhat is text categorization?\n \n\nWhy text categorization?\n \n\nHow to do text categorization?\n \n\nGenerative probabilistic models\n \n\nDiscriminative approaches\n \n\nHow to evaluate categorization results? \n \n \n2\n \n",
    "2": "General Evaluation Methodology\n \n\nHave humans to create a test collection where every document is \n\n \n\nGenerate categorization results using a system on the test collection\n \n\nCompare \nthe system categorization decisions with the human\n-\nmade \ncategorization decisions and quantify their similarity (or equivalently \ndifference)\n \n\nThe higher the similarity is, the better the results are\n \n\nSimilarity can be measured from different perspectives to understand \nthe quality of results in detail (e.g., which category performs better?) \n \n\nIn general, different categorization mistakes may have a different cost \nthat inevitably depends on specific applications, but it is okay not to \nconsider such a cost variation for \nrelative comparison of methods \n \n3\n \n",
    "3": "4\n \nClassification Accuracy (Percentage of Correct Decisions)\n \n \n \nc\n1 \n \n \nc\n2\n \n \n \nc\n3\n   \n\n \n \nc\nk\n \nd\n1\n \ny(+)\n \ny(\n-\n)\n \nn(+)   \n \nn(+)\n \nd\n2 \n \ny(\n-\n)\n \nn(+)\n \ny(+)\n \n          \nn\n(+)\n \nd\n3 \n \nn(+)   \nn\n(+)   \ny\n(+)     \n \nn\n(+)\n \n\n \nd\nN\n \n       \n\n \n+/\n-\n   \nhuman answer\n \n(+= correct; \n-\n \n=incorrect)\n \ny/n    system \nresult\n \n \n(y=yes; n=no)\n \nTotal number of decisions made\n \nTotal number of  correct decisions\n \nClassification Accuracy = \n \n",
    "4": "Problems with Classification Accuracy\n \n\nS\nome decision errors are more serious than others \n \n\nIt may be more important to get the decisions right on some \ndocuments than others\n \n\nIt may be more important to get the decisions right on some \ncategories than others\n \n\nE.g., \ns\npam \nfiltering: \nmissing \na legitimate email costs more than letting \na spam go\n \n\nProblem with imbalanced test set\n \n\nSkewed test set: 98% in category 1; 2% in category 2\n \n\nStrong baseline: put all instances in category 1 \n\n \n98% accuracy!\n \n \n \n \n5\n \n",
    "5": "6\n \nPer\n-\nDocument Evaluation \n \n \n \nc\n1 \n \n \nc\n2\n \n \n \nc\n3\n   \n\n \n \nc\nk\n \nd\n1\n \ny(+)\n \ny(\n-\n)\n \nn(+)   \n \nn(+)\n \nd\n2 \n \ny(\n-\n)\n \nn(+)\n \ny(+)\n \n          \nn\n(+)\n \nd\n3 \n \nn(+)   \nn\n(+)   \ny\n(+)     \n \nn(+)\n \nHow good are the decisions on d\ni\n?\n \n\n \nSystem\n \n\n \nHuman\n \n(+)\n \nTrue\n \nPositives\n \n     \nTP\n \nFalse\n \nNegatives\n \n          \n \nFN\n \nHuman (\n-\n)\n \nFalse\n \nPositives\n \n     \nFP\n \nTrue\n \nNegatives\n \n         \nTN\n \nRecall\n \nDoes the doc have all the categories\n \nit should have? \n \nPrecision\n \n\n \nhow many are correct? \n \n",
    "6": "7\n \nPer\n-\nCategory Evaluation \n \n \n \nc\n1 \n \n \nc\n2\n \n \n \nc\n3\n   \n\n \n \nc\nk\n \nd\n1\n \ny(+)\n \ny(\n-\n)\n \nn(+)   \n \nn(+)\n \nd\n2 \n \ny(\n-\n)\n \nn(+)\n \ny(+)\n \n          \nn\n(+)\n \nd\n3 \n \nn(+)   \nn\n(+)   \ny\n(+)     \n \nn(\n-\n \n)\n \nHow good are the decisions on c\ni\n?\n \n\n \nSystem\n \n\n \nHuman\n \n(+)\n \nTrue\n \nPositives\n \n     \nTP\n \nFalse\n \nNegatives\n \n          \n \nFN\n \nHuman (\n-\n)\n \nFalse\n \nPositives\n \n     \nFP\n \nTrue\n \nNegatives\n \n         \nTN\n \nPrecision\n \nRecall\n \n\n \nhow many are correct? \n \nHas the category been assigned to\n \nall the docs of this category? \n \n",
    "7": "Combine Precision and Recall: F\n-\nMeasure\n \n8\n \nP\n: precision\n \nR\n: recall\n \n\n: \nparameter \n(often set to 1)\n \nWhy not 0.5*P+0.5*R? \n \n\n-\ndoc pairs?\n \n"
}