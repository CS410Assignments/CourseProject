{
    "0": "Text Categorization: \n \nGenerative Probabilistic Models\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Overview\n \n\nWhat is text categorization?\n \n\nWhy text categorization?\n \n\n \nHow to do text categorization?\n \n\nGenerative probabilistic models\n \n\nDiscriminative approaches\n \n\nHow to evaluate categorization results? \n \n \n2\n \n",
    "2": "Document Clustering Revisited\n \n3\n \n\n \np\n(w|\n\n1\n)\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \n\n \ntravel  0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \n\n \np\n(w|\n\n2\n)\n \np\n(w|\n\nk\n)\n \np(\n\n1\n)\n \np(\n\n2\n)\n \np(\n\nk\n)\n \nd\n \nd\n \nd\n \nWhich \ncluster does d belong \nto? \n\n \nWhich \n\ni\n \nwas used to generate d?\n \nd=x\n1\nx\n2\n\nx\nL\n \n  \nwhere \nx\ni\n\nV\n \n",
    "3": "Text Categorization with Na\u00efve Bayes Classifier\n \n\n \np\n(w|\n\n1\n)\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \n\n \ntravel  0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \n\n \np\n(w|\n\n2\n)\n \np\n(w|\n\nk\n)\n \np(\n\n1\n)\n \np(\n\n2\n)\n \np(\n\nk\n)\n \nd\n \nd\n \nd\n \nIF \n\ni\n \nrepresents category i accurately, \n\n \nd=x\n1\nx\n2\n\nx\nL\n \n  \nwhere \nx\ni\n\nV\n \nHow can we make this happen?\n \n",
    "4": "Learn from the Training Data\n \n\n \ns\nports  \n?\n \ng\name   \n?\n \nb\nasketball \n?\n \nf\nootball   \n?\n \n\n \nscience  \n?\n \nscientist   \n?\n \ns\npaceship \n?\n \n\n \ntravel  \n?\n \nattraction   \n?\n \nt\nrip       \n?\n \n\n \np(\n\n1\n)=?\n \np(\n\n2\n)=?\n \np(\n\nk\n)=?\n \nCategory 1\n \nCategory 2\n \nCategory k\n \nCategory 1\n \nCategory 2\n \nCategory k\n \nTraining Documents with Known Categories\n \np\n(w|\n\n1\n)=?\n \np\n(w|\n\n2\n)=?\n \np\n(w|\n\nk\n)=?\n \n",
    "5": "How to Estimate \np(w\n|\n\ni\n) and p(\n\ni\n)\n \n\n \ns\nports  \n?\n \ng\name   \n?\n \nb\nasketball \n?\n \nf\nootball   \n?\n \n\n \nscience  \n?\n \nscientist   \n?\n \ns\npaceship \n?\n \n\n \ntravel  \n?\n \nattraction   \n?\n \nt\nrip       \n?\n \n\n \np(\n\n1\n)=?\n \np(\n\n2\n)=?\n \np(\n\nk\n)=?\n \nCategory 1\n \nCategory 2\n \nCategory k\n \nCategory 1\n \nCategory 2\n \nCategory k\n \nTraining Documents with Known Categories\n \np\n(w|\n\n1\n)=?\n \np\n(w|\n\n2\n)=?\n \np\n(w|\n\nk\n)=?\n \n",
    "6": "Na\u00efve Bayes Classifier: \np(\n\ni\n)=?\n \nand \np(w\n|\n\ni\n)=? \n \nCategory 1\n \nCategory 2\n \nCategory k\n \nWhich category is most popular?\n \nWhich word is most frequent in category i?\n \nWhat are the constraints on \np(\n\ni\n) and\n \np(w|\n\ni\n)?\n \n",
    "7": "Smoothing in Na\u00efve Bayes \n \n\nWhy smoothing? \n \n\nAddress data sparseness (training data is small \n\n \nzero prob.)\n \n\nIncorporate prior knowledge \n \n\nAchieve discriminative weighting (i.e., IDF weighting)\n \n\nHow?\n \nWhat if \n\n?\n \np(w|\n\nB\n): background LM\n \nWhat if \n\n\n?\n \np(w\n|\n\nB\n)=1/|V|?\n \n \n",
    "8": "Anatomy of Na\u00efve Bayes Classifier\n \n9\n \nTwo categories: \n\n1\n \nand \n\n2\n \n \nSum over all words \n \n(features {f\ni\n} )\n \nWeight on each\n \nword (feature) \n\ni\n \n \nCategory bias (\n\n0\n)\n \n\n \nGeneralize\n \n= Logistic Regression!\n \n \n   \nFeature value: f\ni\n=c(\nw,d\n)\n \n"
}