{
    "0": "Text Clustering: \n \nSimilarity\n-\nbased Approaches\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Overview\n \n\nWhat is text clustering?\n \n\nWhy text clustering?\n \n\n \nHow to do text clustering?\n \n\nGenerative probabilistic models\n \n\nSimilarity\n-\nbased approaches\n \n\nHow to evaluate clustering results? \n \n \n2\n \n",
    "2": "3\n \nSimilarity\n-\nbased Clustering: General Idea\n \n\nExplicitly define a similarity function to measure similarity between \n\n \n\nFind an optimal partitioning of data to \n \n\nmaximize intra\n-\ngroup similarity and \n \n\nminimize inter\n-\ngroup similarity \n \n\nTwo strategies for obtaining optimal clustering\n \n\nProgressively construct a hierarchy of clusters (hierarchical clustering)\n \n\nBottom\n-\nup (agglomerative): gradually group similar objects into larger \nclusters\n \n\nTop\n-\ndown (divisive): gradually partition the data into smaller \nclusters\n \n\n\nclustering, e.g., k\n-\nMeans)\n \n",
    "3": "4\n \nSimilarity\n-\nbased Clustering Methods\n \n\nMany general clustering methods are available!\n \n \n \n\nTwo representative methods \n \n\nHierarchical Agglomerative Clustering (HAC)\n \n\nk\n-\nmeans \n \n",
    "4": "5\n \nAgglomerative Hierarchical Clustering\n \n\nGiven a similarity function to measure similarity \nbetween two objects  \n \n\nGradually group similar objects together in a bottom\n-\nup \nfashion to form a hierarchy\n \n\nStop when some stopping criterion is met\n \n\nVariations: different ways to compute group similarity \nbased on individual object similarity\n \n",
    "5": "6\n \nSimilarity\n-\ninduced Structure\n \n",
    "6": "7\n \nHow to Compute Group Similarity\n \nThree popular methods:\n \n \nGiven two groups g1 and g2,\n \n \n\nSingle\n-\nlink algorithm: s(g1,g2)= similarity of the \nclosest\n \npair\n \n\nC\nomplete\n-\nlink algorithm: s(g1,g2)= similarity of the \nfarthest\n \npair\n \n\nA\nverage\n-\nlink algorithm: s(g1,g2)= \naverage\n \nof similarity of \nall pairs\n \n \n \n",
    "7": "8\n \nGroup Similarity Illustrated\n \nSingle\n-\nlink algorithm\n \n?\n \ng1\n \ng2\n \ncomplete\n-\nlink algorithm\n \n\n \naverage\n-\nlink algorithm\n \n",
    "8": "9\n \nComparison of Single\n-\nLink, Complete\n-\nLink, and \nAverage\n-\nLink\n \n\nSingle\n-\nlink\n \n\n\n \n\nIndividual decision, sensitive to outliers \n \n\nComplete\n-\nlink\n \n\n\n \n\nIndividual decision, sensitive to outliers\n \n\nAverage\n-\nlink \n \n\n\n \n\nGroup decision, insensitive to outliers\n \n\nWhich one is the best? It depends on what you need! \n \n",
    "9": "10\n \nK\n-\nMeans Clustering\n \n\nRepresent each text object as a term vector and assume a similarity \nfunction defined on two objects\n \n\nStart with k randomly selected vectors and assume they are the \ncentroids of k clusters (initial tentative clustering)\n \n\nAssign every vector to a cluster whose centroid is the closest to the \nvector\n \n\nRe\n-\ncompute the centroid for each cluster based on the newly assigned \nvectors in the cluster\n \n\nRepeat this process until the similarity\n-\nbased objective function (i.e., \nwithin cluster sum of squares) converges  (to a local minimum)\n \n \nVery similar to clustering with EM for mixture model! \n \n\n \nInitialization\n \n\n \nE\n-\nstep     difference?\n \n\n \nM\n-\nstep     difference?\n \n",
    "10": "Summary of Clustering Methods\n \n\nModel based approaches (mixture model) \n \n\nUses an implicit similarity function (model \n\n \nclustering bias)\n \n\n\n \n\nComplex generative models can discover complex structures\n \n\nPrior can be leveraged to further customize the clustering algorithm\n \n\nHowever, no easy way to directly control the similarity measure\n \n\nSimilarity\n-\nbased approaches \n \n\nAllows for direct and flexible specification of similarity\n \n\nObjective function to be optimized is not always clear\n \n\n \nBoth approaches can generate both term clusters and doc clusters\n \n \n11\n \n"
}