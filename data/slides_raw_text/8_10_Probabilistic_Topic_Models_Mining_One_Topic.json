{
    "0": "Probabilistic Topic Models: \n \nMining One Topic \n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Probabilistic Topic Models: Mining One Topic\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "Simplest Case of Topic Model: Mining One Topic\n \n3\n \nDoc d\n \n\n \n100%\n \nINPUT:  C={d}, V\n \nOUTPUT: { \n\n}\n \nText Data\n \ntext  ?\n \nmining   ?\n \nassociation ?\n \ndatabase   ? \n \n\n \nquery      ?   \n \n\n \nP(w|\n\n)\n \n",
    "3": "Language Model Setup\n \n\nData\n: Document d= x\n1\n \nx\n2\n \n\n|d| \n, x\ni\n \n\nV={w\n1\n \n\nM\n}\n \nis a word\n \n \n\nModel\n: Unigram LM \n\n(=topic) : \n{\n\ni\n=p\n(\nw\ni\n \n|\n\n)}, \ni\n\n\n1\n\n\nM\n=1\n \n \n\nLikelihood\n \nfunction:\n \n \n \n \n \n\nML\n \nestimate:\n \n4\n \n",
    "4": "Normalized \n \nCounts\n \nComputation of Maximum Likelihood Estimate\n \nUse Lagrange multiplier approach\n \nMaximize p(d|\n\n)\n \nMax. Log\n-\nLikelihood\n \nSubject to constraint:\n \n",
    "5": "Can we get rid of\n \nthese common words?\n \nWhat Does the Topic \nL\nook \nL\nike?\n \n6\n \nthe 0.031\n \na 0.018\n \n\n \ntext  0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering 0.005\n \ncomputer 0.0009\n \n\n \nfood 0.000001\n \n\n \nText mining\n \npaper\n \nd\n \np(w|\n \n\n)\n \n"
}