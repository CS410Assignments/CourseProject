{
    "0": "Probabilistic Topic Models: \n \nExpectation\n-\nMaximization Algorithm \n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n",
    "1": "Probabilistic Topic Models: \n \nExpectation\n-\nMaximization (EM) Algorithm\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n2\n \n",
    "2": "Estimation of One Topic\n: \nP(w|\n \n\nd\n)\n \ntext  ?\n \nmining \n?\n \nassociation \n?\n \nclustering \n?\n \n\n \nthe\n \n?\n \n\nd\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  0.000006\n \n\nB\n \nP(\n\nd\n)=0.5\n \nP(\n\nB\n)=0.5\n \nTopic \n \nChoice\n \np(\n\nd \n)+(\n\nB\n)=1\n \n\ntext mining...\n \ni\n\nclustering\n\n\nText.. \nthe\n \n \n \nd\n \nHow to set \n\nd\nto\n \nmaximize p(d|\n\n)?\n \n(all other parameters are known)\n \n3\n \n",
    "3": "\n \ntext  ?\n \nmining \n?\n \nassociation \n?\n \nclustering \n?\n \n\n \nthe\n \n?\n \n\nd\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  0.000006\n \n\nB\n \nP(\n\nd\n)=0.5\n \nP(\n\nB\n)=0.5\n \nTopic \n \nChoice\n \np(\n\nd \n)+(\n\nB\n)=1\n \np(w|\n \n\nB\n)\n \n\ntext mining...\n \ni\n\nclustering\n\n\nText.. \nthe\n \n \n \nd\n \n\n \n4\n \n",
    "4": "Given all the parameters, infer the distribution a \n\n \ntext  0.04\n \nmining \n0.035\n \nassociation 0.03\n \nclustering \n0.005\n \n\n \nthe\n \n0.000001\n \n\nd\n \n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe \n0.01\n \nfood \n0.003\n \n\n \ntext  0.000006\n \n\nB\n \nP(w|\n \n\nd\n)\n \np(w|\n \n\nB\n)\n \nP(\n\nd\n)=0.5\n \nP(\n\nB\n)=0.5\n \nTopic \n \nChoice\n \np(\n\nd \n)+p(\n\nB\n)=1\n \n\ntext\n\nfrom \n\nd\n \nor \n\nB\n \n?\n \np\n(\n\nd\n\n\nd\n) \n \nFrom \n\nd\n \n(Z=0)?\n \np\n(\n\nB\n)p\n\n\nB\n) \n \nFrom \n\nB \n(Z=1)?\n \n5\n \n",
    "5": "The Expectation\n-\nMaximization (EM) Algorithm\n \nHidden Variable\n: \n \nz \n\n{0, 1}\n \nthe\n \npaper\n \npresents\n \na\n \ntext\n \nmining\n \nalgorithm\n \nfor\n \nclustering\n \n...\n \nz\n \n1\n \n1\n \n1\n \n1\n \n0\n \n0\n \n0\n \n1\n \n0\n \n...\n \nInitialize \np(w\n|\n\nd \n) with \nrandom \nvalues. \n \n      \nThen iteratively improve it using E\n-\nstep & M\n-\nstep. \n \n              \n\n \nE\n-\nstep\n \nM\n-\nstep\n \nHow likely w\n \nis from \n\nd\n \n \n6\n \n",
    "6": "EM Computation in Action\n \nE\n-\nstep\n \nM\n-\nstep\n \nAssume\n \n \np\n(\n\nd \n)=p(\n\nB\n)= 0.5\n \nand p(w|\n\nB\n) is known\n \nLikelihood increasing\n \n\n \n7\n \n",
    "7": "EM As Hill\n-\nClimbing \n\n \nConverge to Local Maximum\n \n \nLikelihood \np(d| \n\n)\n \n\n \ncurrent guess\n \nLower \nbound of likelihood function\n \nnext guess\n \nOriginal likelihood\n \nM\n-\nstep = maximizing the lower bound\n \nE\n-\nstep = computing the lower \nbound\n \n8\n \n",
    "8": "Summary\n \n\nExpectation\n-\nMaximization (EM) algorithm \n \n\nGeneral algorithm for computing ML estimate of mixture models\n \n\nHill\n-\nclimbing, so can only converge to a local maximum (depending \non initial points)\n \n\nE\n-\n\nvariables \n \n\nM\n-\nstep: \n\n\n \n\n\n\n \nSplit counts of events \nprobabilistically  \n \n9\n \n"
}