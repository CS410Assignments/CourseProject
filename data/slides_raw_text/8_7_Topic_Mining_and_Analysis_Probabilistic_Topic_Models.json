{
    "0": "Topic Mining and Analysis: \n \nProbabilistic Topic Models\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Topic Mining and Analysis: \n \nProbabilistic Topic Models\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "\n \n\nLack of expressive power\n \n\nCan only represent simple/general topics\n \n\nC\n\n \n\nIncompleteness in vocabulary coverage\n \n\n\n \n\nWord sense ambiguity \n \n\nA topical term or related term can be ambiguous (e.g., basketball \nstar vs. star in the sky)\n \n3\n \n\n \nTopic = {Multiple Words}\n \n+\n  \nweights on words\n \n\n \nSplit an ambiguous word \n \nA probabilistic topic model can do all these!\n \n",
    "3": "Improved Idea: Topic = Word Distribution\n \n4\n \n\nSports\n\n \n\nTravel\n\n \n\nScience\n\n \n\n \n\n1\n \n\n2\n \n\nk\n \nP(w|\n\nk\n)\n \nP(w|\n\n2\n)\n \ntravel  \n0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \nflight   0.004\n \nhotel      0.003\n \nisland     0.003\n \n\n \nculture      0.001\n \n\n \nplay\n      \n0.0002\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \ntelescope   0.004\n \ngenomics   0.004\n \nstar\n    \n0.002\n \n\n \ngenetics    0.001\n \n\n \nt\nravel \n     \n0.00001\n \n\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \nplay\n      \n0.003\n \ns\ntar\n       \n0.003\n \n\n \nnba\n      \n0.001\n \n\n \nt\nravel\n      \n0.0005\n \n\n \nP(w|\n\n1\n)\n \n\n \n",
    "4": "Probabilistic Topic Mining and Analysis\n \n\nInput\n \n\nA \ncollection\n \nof  \nN\n \ntext documents \nC={d\n1\n\nd\nN\n}\n \n\nVocabulary set: \nV={w\n1\n\nM\n}\n \n\nNumber of topics\n: \nk\n \n\nOutput\n \n\nk topics, each a word distribution\n: \n{ \n\n1\n\n\nk\n \n}\n \n\nCoverage of topics in each d\ni\n: \n{ \n\ni1\n\n \n\nik\n \n}\n \n\n\nij\n=prob. of d\ni\n \ncovering topic \n\nj\n \n \n \n5\n \n",
    "5": "The Computation Task\n \n6\n \n\n \nDoc 2\n \nDoc N\n \n\n \nDoc 1\n \n\n1\n \n\n2\n \n\nk\n \n\n11\n \n\n12\n \n\n1k\n \n\n21\n=0%\n \n\n2\n2\n \n\n2\nk\n \n\nN1\n=0%\n \n \n\nN\n2\n \n\nN\nk\n \n30%\n \n1\n2\n%\n \n8\n%\n \ns\nports  0.02\n \ng\name   0.01\n \nb\nasketball 0.005\n \nf\nootball   0.004\n \n\n \nscience  0.04\n \nscientist   0.03\n \ns\npaceship 0.006\n \n\n \ntravel  0.05\n \nattraction   0.03\n \nt\nrip       0.01\n \n\n \nINPUT:  C, k, V\n \nOUTPUT: { \n\n1\n\n\nk\n \n}, \n{ \n\ni1\n\n\nik\n \n}\n \nText Data\n \n",
    "6": "Generative Model for Text Mining\n \n7\n \nModeling of Data Generation: P(Data |Model, \n\n)\n \n\n=(\n{ \n\n1\n\n\nk\n \n}, \n{ \n\n11\n\n\n1k\n \n\n{ \n\nN1\n\n\nNk\n \n})\n \nParameter Estimation/ Inferences \n \n \n\n*\n \n= \nargmax\n \n\n \np(Data|\n \nModel, \n\n)\n \nP(Data |Model, \n\n)\n \n\n \n\n*\n \nHow many parameters in total?\n \n",
    "7": "Summary\n \n\nTopic represented as word distribution\n \n\nMultiple words: allow for describing a complicated topic\n \n\nWeights on words: model subtle semantic variations of a topic\n \n\nTask of topic mining and analysis \n \n\nInput: collection C, number of topics k, vocabulary set V\n \n\nOutput: a set of topics, each a word distribution; coverage of all \ntopics in each document\n \n8\n \n\n=(\n{ \n\n1\n\n\nk\n \n}, \n{ \n\n11\n\n\n1k\n \n\n{ \n\nN1\n\n\nNk\n \n})\n \n",
    "8": "Summary \n(cont.)\n \n\nGenerative model \nfor text mining\n \n\nModel data generation \nwith a prob. model:  \nP(Data \n|Model, \n\n)\n \n\nInfer the most likely parameter values \n\n*\n \ngiven a particular \ndata set\n:   \n\n*\n \n= \nargmax\n \n\n \np(Data|\n \nModel, \n\n)\n \n\nTake \n\n*\n \n\n \nto be mined for the text mining \nproblem\n \n\nAdjust \nthe design of the model to discover different knowledge \n \n9\n \n"
}