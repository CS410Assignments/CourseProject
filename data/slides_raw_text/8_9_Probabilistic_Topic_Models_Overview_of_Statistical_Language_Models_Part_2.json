{
    "0": "Probabilistic Topic Models: \n \nOverview of Statistical Language Models\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n",
    "1": "Probabilistic Topic Models: \n \nOverview of Statistical Language Models\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n2\n \n",
    "2": "What \nIs \na Statistical Language Model (LM)?\n \n\nA probability distribution over word sequences\n \n\n\nToday is Wednesday\n\n\n \n0.001\n \n\n\nToday Wednesday is\n\n\n \n0.0000000000001\n \n\n\nThe eigenvalue is positive\n\n) \n\n \n0.00001\n \n\nContext\n-\ndependent! \n \n\nCan also be regarded as a probabilistic mechanism for \n\n\n \n\n \n \nToday is Wednesday\n \n\n \nToday Wednesday is \n \nThe eigenvalue is positive\n \n3\n \n",
    "3": "The Simplest Language Model: Unigram LM\n \n\nGenerate text by generating each word INDEPENDENTLY\n \n\nThus, p(w\n1\n \nw\n2\n \n... \nw\nn\n)=p(w\n1\n)p(w\n2\n\nw\nn\n)\n \n\nParameters: {p(\nw\ni\n)}  p(w\n1\n\nw\nN\n)=1 (N is voc. size)\n \n\nText = sample drawn according to this \nword distribution\n \ntoday\n \neigenvalue\n \nWednesday\n \n \n\n \np\n\n\n \n    \n= \np\n\n\n \n    \n= 0.0002 \n\n \n0.001 \n\n \n0.000015 \n \n4\n \n",
    "4": "Text Generation with Unigram LM \n \nUnigram LM  p(w|\n\n)\n \n                     \n \n\n \ntext  0.2\n \nmining 0.1\n \nassociation 0.01\n \nclustering 0.02\n \n\n \nfood\n \n0.00001\n \n\n \nTopic 1:\n \nText mining\n \n\n \nfood 0.25\n \nnutrition 0.1\n \nhealthy 0.05\n \ndiet 0.02\n \n\n \nTopic 2:\n \nHealth\n \n \nDocument d\n \np(d|\n \n\n)=?\n \nText mining\n \npaper\n \nFood nutrition\n \npaper\n \nSampling\n \n5\n \n",
    "5": "Estimation of Unigram LM \n \nUnigram LM  p(w|\n\n)=?\n \n                     \n \nText Mining Paper  d\n \nEstimation\n \n \ntext \n10\n \nmining 5\n \nassociation 3\n \ndatabase 3\n \nalgorithm 2\n \n\n \nquery 1\n \nefficient 1\n \n \n\n \n \ntext  ? \n \n \nmining \n?\n \n \nassociation \n?\n \n \ndatabase \n?\n \n \n\n \n \nquery \n?\n \n\n \n \nT\notal \n#\nwords=\n100\n \n \n10/100\n \n5/100\n \n3/100\n \n3/100\n \n \n1/100\n \nIs this \nour best estimate?\n \n\n \nMaximum Likelihood \n \nEstimate\n \n6\n \n",
    "6": "Maximum Likelihood vs. Bayesian\n \n\nMaximum likelihood estimation\n \n\n\n \n \n\nProblem: Small sample\n \n\nBayesian estimation: \n \n\n\nexplaining data well\n \n \n\nProblem: How to define prior?\n \nBayes Rule\n \nMaximum a Posteriori (MAP) estimate\n \n7\n \n",
    "7": "Illustration of Bayesian Estimation\n \nPrior: p(\n\n)\n \nLikelihood:\n \n \np(X|\n\n)\n \nX=(x\n1\n\nx\nN\n)\n \nPosterior:\n \n \np(\n\n|X)\n\n \np(X|\n\n)\np(\n\n)\n \n \n\n \n\n0\n: prior mode \n \n\nml\n: ML estimate\n \n\n1\n: \nposterior mode \n \nBayesian inference: f(\n\n)=?  \n \nPosterior \n \nMean\n \n8\n \n",
    "8": "Summary\n \n\nLanguage Model \n= probability distribution over text = generative \nmodel for text data  \n \n\nUnigram\n \nLanguage Model = \nw\nord distribution\n \n\nLikelihood \nfunction: \np(X|\n\n)\n \n\nGiven \n\n \n\n \nwhich X has a higher likelihood?\n \n\nGiven X \n\n \nwhich \n\n \nmaximizes p(X|\n \n\n)?  \n[ML estimate]\n \n\nBayesian \nestimation/inference\n \n\nMust define a \nprior: p\n(\n\n)\n \n\nPosterior \ndistribution\n: \np(\n\n|X)\n\n \np(X|\n\n)\np(\n\n)\n \n\n \n\n\n!\n \n \n \n9\n \n"
}