{
    "0": "Text Clustering: \n \nGenerative Probabilistic Models (Part \n3\n)\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Text Clustering: Generative Probabilistic Models \n(Part 3)\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "How Can \nW\ne Compute the ML Estimate? \n \n\nData: a collection of documents C={d\n1\n\nd\nN\n}\n \n\nModel: mixture of k unigram LMs: \n\n=(\n{\n\ni\n}; {p(\n\ni\n)}), \ni\n\n[1,k]\n \n\nTo generate a document, first \nchoose a \n\ni\n \naccording to  \np(\n\ni\n) and then \ngenerate \nall\n \nwords in the document using p(w|\n\ni\n)\n \n\nLikelihood: \n \n \n \n \n\nMaximum Likelihood estimate\n \n3\n \n",
    "3": "EM Algorithm for Document Clustering\n \n\nInitialization: Randomly set \n\n=(\n{\n\ni\n}; {p(\n\ni\n)}), \ni\n\n[1,k\n]\n \n\nRepeat until likelihood p(C|\n\n) converges\n \n\nE\n-\nStep: Infer which distribution has been used to generate \ndocument d: hidden variable \nZ\nd\n \n\n[1, k\n] \n \n \n \n\nM\n-\nStep: \nR\ne\n-\nestimation of all parameters\n \n4\n \n",
    "4": "An Example of 2 Clusters\n \n5\n \nRandom Initialization\n \nE\n-\nstep\n \nHidden variables: \n \nZ\nd\n \n\n{1, 2}\n \np\n(\n\n1\n \n)=\np(\n\n2\n \n)= 0.5\n \n \np(w|\n\n1\n \n)\n \n \np(w|\n\n2\n \n)\n \n \ntext\n \n0.5\n \n0.1\n \nmining\n \n0.2\n \n0.1\n \nmedical\n \n0.2\n \n0.75\n \nhealth\n \n0.1\n \n0.05\n \nc(\nw,d\n)\n \ntext\n \n2\n \nmining\n \n2\n \nmedical\n \n0\n \nhealth\n \n0\n \nDocument d\n \n",
    "5": "Normalization to Avoid Underflow\n \n6\n \np(w|\n\n1\n \n)\n \n \np(w|\n\n2\n \n)\n \n \ntext\n \n0.5\n \n0.1\n \n(0.5+0.1)/2\n \nmining\n \n0.2\n \n0.1\n \n(0.2+0.1)/2\n \nmedical\n \n0.2\n \n0.75\n \n(0.2+0.75)/2\n \nhealth\n \n0.1\n \n0.05\n \n(0.1+0.05)/2\n \nAverage of p(w|\n\ni\n \n) \n \nas a possible normalizer\n \n",
    "6": "An Example of 2 Clusters (cont.)\n \n7\n \nM\n-\nStep\n \nFrom E\n-\nStep\n \np\n(\n\n1\n \n)=? \np\n(\n\n2\n \n)= ?\n \np(w|\n\n1\n \n)\n \n \np(w|\n\n2\n \n)\n \n \ntext\n \nmining\n \nmedical\n \nhealth\n \nP(Z\nd\n=1|d)\n \nd1\n \n0.9\n \nd2\n \n0.1\n \nd3\n \n0.8\n \n?\n \n?\n \n?\n \n?\n \n?\n \n?\n \n?\n \n?\n \n\n \n\n \nd1\n \n2\n \n3\n \nd2\n \n1\n \n2\n \nd3\n \n4\n \n3\n \n",
    "7": "Summary of Generative Model for Clustering\n \n\nA slight variation of topic model can be used for clustering \ndocuments \n \n \n\nEach \ncluster\n \nis represented by a \nunigram LM \np(w|\n\ni\n \n) \n\n \nTerm cluster\n \n\nA document is generated by first choosing a unigram LM and then \ngenerating \nALL words \nin the document using this \nsingle LM \n \n\nEstimated model parameters give both a topic characterization of each \ncluster and a probabilistic assignment of a document into each cluster\n \n\n\ncorresponding to the unigram LM most likely used to generate the \ndocument\n \n\nEM algorithm can be used to compute the ML estimate \n \n\nNormalization is often needed to avoid underflow \n \n8\n \n"
}