{
    "0": "Probabilistic Retrieval Model: \n \nStatistical Language Model\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Probabilistic Retrieval Model: Statistical Language Model\n \n",
    "2": "Overview\n \n\nWhat is a Language Model? \n \n\nUnigram Language Model\n \n\nUses of a Language Model\n \n3\n \n",
    "3": "4\n \nWhat is a Statistical Language Model (LM)?\n \n\nA probability distribution over word sequences\n \n\n\nToday is Wednesday\n\n\n \n0.001\n \n\n\nToday Wednesday is\n\n\n \n0.0000000000001\n \n\n\nThe eigenvalue is positive\n\n) \n\n \n0.00001\n \n\nContext\n-\ndependent! \n \n\nCan also be regarded as a probabilistic mechanism for \n\n \n \nToday is Wednesday\n \n\n \nToday Wednesday is \n \nThe eigenvalue is positive\n \n",
    "4": "Why is a LM Useful?\n \n\nQ\nuantify the uncertainties \nin\n \nnatural language\n \n\nAllows us to answer questions like:\n \n\n\nJohn\n\nfeels\n\n\nhappy\n\nhabit\n\n \n(\nspeech \nrecognition\n)\n \n\n\n\ntext \ncategorization, information retrieval\n)\n \n\nGiven that a user is interested in sports news, how likely would the \n\ninformation retrieval\n)\n \n \n5\n \n",
    "5": "6\n \nThe Simplest Language Model: Unigram LM\n \n\nGenerate text by generating each word INDEPENDENTLY\n \n\nThus, p(w\n1\n \nw\n2\n \n... \nw\nn\n)=p(w\n1\n)p(w\n2\n\nw\nn\n)\n \n\nParameters: {p(\nw\ni\n)}  p(w\n1\n\nw\nN\n)=1 (N is voc. size)\n \n\nText = sample drawn according to this \nword distribution\n \ntoday\n \neigenvalue\n \nWednesday\n \n \n\n \np\n\n\n \n    \n= \np\n\n\n \n    \n= 0.0002 \n\n \n0.001 \n\n \n0.000015 \n \n",
    "6": "Text Generation with Unigram LM \n \n7\n \nUnigram LM  p(w|\n\n)\n \n                     \n \n\n \ntext  0.2\n \nmining 0.1\n \nassociation 0.01\n \nclustering 0.02\n \n\n \nfood\n \n0.00001\n \n\n \nTopic 1:\n \nText mining\n \n\n \nfood 0.25\n \nnutrition 0.1\n \nhealthy 0.05\n \ndiet 0.02\n \n\n \nTopic 2:\n \nHealth\n \n \nDocument =?\n \nText mining\n \npaper\n \nFood nutrition\n \npaper\n \nSampling\n \n",
    "7": "Estimation of Unigram LM \n \n8\n \nUnigram LM  p(w|\n\n)=?\n \n                     \n \nText Mining Paper  d\n \nEstimation\n \n \ntext \n10\n \nmining 5\n \nassociation 3\n \ndatabase 3\n \nalgorithm 2\n \n\n \nquery 1\n \nefficient 1\n \n \n\n \n \ntext  ? \n \n \nmining \n?\n \n \nassociation \n?\n \n \ndatabase \n?\n \n \n\n \n \nquery \n?\n \n\n \n \nT\notal \n#\nwords=\n100\n \n \n10/100\n \n5/100\n \n3/100\n \n3/100\n \n \n1/100\n \nIs this the best estimate? \n \nMaximum Likelihood (ML) Estimator:\n \n",
    "8": "LMs for Topic Representation\n \n9\n \nGeneral Background\n \nEnglish Text\n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe 0.01\n \n...\n \nfood 0.003\n \ncomputer 0.00001\n \ntext  0.000006\n \n\n \nB\n \nBackground\n \nLM\n:\n \np(\nw|B\n)\n \nComputer Science\n \nPapers\n \nthe 0.032\n \na 0.019\n \nis 0.014\n \nwe 0.011\n \n...\n \ncomputer 0.004\n \nsoftware 0.0001\n \ntext  0.00006\n \n\n \nCollection\n \nLM\n: \np(\nw|C\n)\n \nC\n \nthe 0.031\n \n\n \ntext  0.04\n \nmining 0.035\n \nassociation 0.03\n \nclustering 0.005\n \ncomputer 0.0009\n \n\n \nfood 0.000001\n \nText mining\n \npaper\n \nDocument\n \nLM\n: \np(\nw|d\n)\n \nd\n \n",
    "9": "LMs for Association Analysis\n \n10\n \n\ncomputer\n\n \nthe 0.032\n \na 0.019\n \nis 0.014\n \nwe 0.008\n \ncomputer 0.004\n \nsoftware 0.0001\n \nTopic\n \nLM\n: \np(\n\n\n \nDocuments\n \ncontaining word \n \n\n \nthe 0.03\n \na 0.02\n \nis 0.015\n \nwe 0.01\n \n...\n \ncomputer 0.00001\n \nBackground\n \nLM\n:\n \np(\nw|B\n)\n \nGeneral Background\n \nEnglish Text\n \nB\n \nNormalized Topic\n \nLM\n: \n \np(\n\n\nw|B\n)\n \ncomputer 400\n \nsoftware 150\n \nprogram 104\n \n\n \ntext  3.0\n \n\n \nthe 1.1\n \na 0.99\n \nis 0.9\n \nwe 0.8\n \n",
    "10": "Summary\n \n\nLanguage Model = probability \nd\nistribution over text \n \n\nUnigram Language Model = \nw\nord distribution\n \n\nUses of a Language Model\n \n\nRepresenting topics\n \n\nDiscovering word associations\n \n11\n \n",
    "11": "Additional Readings\n \n\nChris Manning and \nHinrich\n \nSch\u00fctze\n, Foundations of \nStatistical Natural Language Processing,  MIT Press. \nCambridge, MA: May 1999.\n \n\nRosenfeld, R., \"Two decades of statistical language \nmodeling: where do we go from here?,\" \nProceedings of \nthe IEEE\n \n, vol.88, no.8, \npp.1270,1278\n, Aug. \n2000\n \n \n12\n \n"
}