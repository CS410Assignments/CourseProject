{
    "0": "Text Categorization: Evaluation (Part 2)\n \n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "2\n \n(Macro) Average Over \nA\nll the Categories \n \n \n \nc\n1 \n \n \nc\n2\n \n \n \nc\n3\n   \n\n \n \nc\nk\n \nd\n1\n \ny(+)\n \ny(\n-\n)\n \nn(+)   \n \nn(+)\n \nd\n2 \n \ny(\n-\n)\n \nn(+)\n \ny(+)\n \n          \nn\n(+)\n \nd\n3 \n \nn(+)   \nn\n(+)   \ny\n(+)     \n \nn(+)\n \n\n \nd\nN\n         \n\n \nPrecision   \n \n   \nRecall     \n \nF\n-\nMeasure\n \np1   \n \nr1     \n \nf1\n \np2   \n \nr2    \n \nf2\n \n \nOverall Precision\n \n \nAggregate\n \n \nOverall Recall\n \n \nOverall F score\n \np3   \n \nr3    \n \nf3\n \npk\n   \n \nrk\n    \n \nfk\n \n\n \n\n \n\n \n",
    "2": "3\n \n(Macro) Average Over \nA\nll the Documents \n \n \n \nc\n1 \n \n \nc\n2\n \n \n \nc\n3\n   \n\n \n \nc\nk\n \nd\n1\n \ny(+)\n \ny(\n-\n)\n \nn(+)   \n \nn(+)\n \nd\n2 \n \ny(\n-\n)\n \nn(+)\n \ny(+)\n \n          \nn\n(+)\n \nd\n3 \n \nn(+)   \nn\n(+)   \ny\n(+)     \n \nn(+)\n \n\n \nd\nN\n \nPrecision   Recall     F\n-\nMeasure\n \np\n1                \nr\n1           \nf\n1\n \np2                r\n2\n           \nf\n2\n \n \nOverall Recall\n \n \nOverall F score\n \n \nOverall Precision\n \nAggregate\n \np\nN\n                \nr\nN\n           \nf\nN\n \n\n \n",
    "3": "4\n \nMicro\n-\nAveraging of Precision\n \nand Recall \n \n \n \nc\n1 \n \n \nc\n2\n \n \n \nc\n3\n   \n\n \n \nc\nk\n \nd\n1\n \ny(+)\n \ny(\n-\n)\n \nn(+)   \n \nn(+)\n \nd\n2 \n \ny(\n-\n)\n \nn(+)\n \ny(+)\n \n          \nn\n(+)\n \nd\n3 \n \nn(+)   \nn\n(+)   \ny\n(+)     \n \nn\n(+)\n \n\n \nd\nN\n \n       \n\n \n\n \nSystem\n \n\n \nHuman\n \n(+)\n \nTrue\n \nPositives (\n \nTP)\n \nFalse\n \nNegatives \n(\nFN)\n \nHuman (\n-\n)\n \nFalse\n \nPositives(\nFP)\n \nTrue\n \nNegatives\n(TN)\n \nPrecision\n \nRecall\n \nFirst pool all decisions, \n \nt\nhen compute precision and recall\n \n",
    "4": "Sometimes \nR\nanking \nI\ns \nM\nore \nA\nppropriate\n \n\nThe categorization results are often passed to a human\n \nfor \n \n\nfurther editing (e.g., correcting system mistakes on news categories)\n \n\nprioritizing a task (e.g., routing an email to the right person for \nprocessing)\n \n\nIn such cases, we can evaluate the results as a ranked list if the \nsystem can give scores for the decisions\n \n\nE.g., discovery of spam emails (\n\n \n\n \n\nOften more appropriate to frame the problem as a ranking problem \ninstead of a categorization problem (e.g., ranking documents in a \nsearch engine)\n \n5\n \n",
    "5": "Summary of Categorization Evaluation\n \n\nEvaluation is always very important, so get it right! \n \n\nMeasures must reflect the \nintended use \nof the results for a \nparticular application (e.g., spam filtering vs. news \ncategorization)\n \n\nConsider: How will the results be further processed (by a user)?\n \n\nIdeally associate a different cost with each different decision error\n \n\nCommonly used measures for \nrelative\n \ncomparison of different \nmethods:\n \n\nAccuracy, precision, recall, F score\n \n\nVariations: per\n-\ndocument, per\n-\ncategory, micro vs. macro averaging\n \n\nSometimes \nranking\n \nmay be more appropriate \n \n6\n \n",
    "6": "Suggested Reading\n \n\nManning, Chris D., \nPrabhakar\n \nRaghavan\n, and \nHinrich\n \nSch\u00fctze\n. \nIntroduction to Information Retrieval\n. \nCambridge: Cambridge University Press, 2007.  \n(\nChapters 13\n-\n15)\n \n\nYang, \nYiming\n. \n1999. An Evaluation of Statistical \nApproaches to Text Categorization. \nInf. \nRetr\n.\n \n1, 1\n-\n2 \n(May 1999), 69\n-\n90. \nDOI=10.1023/A:1009982220290\n \n \n \n7\n \n"
}