{
    "0": "Syntagmatic Relation Discovery: Entropy\n \n \nChengXiang\n \n\n \nDepartment of Computer Science\n \nUniversity of Illinois at Urbana\n-\nChampaign\n \n1\n \n",
    "1": "Syntagmatic Relation Discovery\n: Entropy\n \n2\n \nReal World\n \nObserved World\n \nText Data\n \n(English)\n \nPerceive\n \nExpress\n \n(Perspective)\n \n3. Topic mining & analysis\n \n4. Opinion mining and \n \n \n   \nsentiment analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  & text \nrepresentation\n \n3. Topic mining and analysis\n \n5\n. Text\n-\nbased prediction\n \n1.\nNatural language \nprocessing  and text \nrepresentation\n \n2. Word association \nmining and analysis\n \n",
    "2": "Syntagmatic Relation = Correlated Occurrences\n \n3\n \nMy \ncat\n \neats\n \nfish\n \non Saturday\n \nHis \ncat\n \neats\n \nturkey\n \non Tuesday\n \nMy \ndog\n \neats\n \nmeat\n \non \nSunday\n \nHis \ndog\n \neats\n \nturkey \non Tuesday\n \n\n \nMy  __  \neats\n \n__  on Saturday\n \nHis  __  \neats\n \n__  on Tuesday\n \nMy \n__   \neats\n \n__  on Sunday\n \nHis __   \neats\n \n__  on Tuesday\n \n\n \nWhat words tend to occur \nto the \nleft\n \nof \n\neats\n\n \nWhat words \n \nare to the \nright? \n \n\neats\n\nother words \nalso tend to occur? \n \n",
    "3": "Word Prediction: Intuition \n \n4\n \nText Segment (any unit, e.g.,  sentence, paragraph, document)\n \nPrediction Question: Is word\n \nW\n \npresent (or absent) in this segment? \n \nAre some words easier to predict than others? \n \n \n\n \n\n \n\n \n",
    "4": "Word Prediction: Formal Definition \n \n5\n \nBinary Random Variable : \n \n \nX\nw\n \n\n{0, 1} \n \nThe more random \nX\nw \n \nis, the more difficult the prediction would be.\n \n\n \nof a random variable like Xw? \n \n",
    "5": "6\n \nEntropy H(X) Measures Randomness of X\n \nP(Xw\n=1) \n \nor equivalently\n \nP(Xw=0)  (Why?)\n \n \n \nH(X\nw\n)\n \n1.0\n \n1.0\n \n0.5\n \nFor what \nX\nw\n \n, does H(\nX\nw\n) \n \nreach \nmaximum\n/\nminimum\n?\n \n   \nE.g., P(\nX\nw\n=1)=1?   \nP(\nX\nw\n=1\n)=0.5? \n \n",
    "6": "7\n \nEntropy H(X): Coin Tossing\n \nP(X=1\n) \n \n \nH(X)\n \n1.0\n \n1.0\n \n0.5\n \nX\ncoin\n:  tossing a coin \n \nFair coin:  p(X=1)=p(X=0)=1/2 \n \nCompletely biased:  p(X=1)=1 \n \n",
    "7": "Entropy for Word Prediction \n \n8\n \nIs word\n \nW\n \npresent (or absent) in this segment? \n \n\nmeat\n\nthe\n\nunicorn\n\n \n\n \n\n \nWhich is \nhigh/low\n?\n  \nH(\nX\nmeat\n), H(\nX\nthe\n), or H(\nX\nunicorn\n)?  \n \nH(\nX\nthe\n)\n\n0   \n\n \nno uncertainty since \np(\nX\nthe\n=1\n)\n \n\n \n1 \n \nHigh entropy words are harder to predict!\n \n"
}