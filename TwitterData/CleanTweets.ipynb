{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emilysallens\\AppData\\Local\\Temp\\ipykernel_20504\\1859075606.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('(\\@\\w+.*?)',\"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    330\n",
       "1.0      8\n",
       "Name: Islamophobic?, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('tweets_labeled.xlsx')\n",
    "df = df[df['Islamophobic?'].notna()]\n",
    "\n",
    "df = df[df[\"text\"].str.contains(\"PBUH\") == False]\n",
    "df = df[df[\"text\"].str.contains(\"P.B.U.H\") == False]\n",
    "df = df[df[\"text\"].str.contains(\"Quran4all\") == False]\n",
    "df = df[df[\"text\"].str.contains(\"Quran Recitation\") == False]\n",
    "df['text'] = df['text'].str.replace('#','')\n",
    "df['text'] = df['text'].str.replace('(\\@\\w+.*?)',\"\")\n",
    "\n",
    "#URL\tText\tIslamophobic?\tSource\tPublication Date \n",
    "\n",
    "print(len(df.index))\n",
    "df['Islamophobic?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459\n",
      "452\n",
      "0.0    652\n",
      "1.0    138\n",
      "Name: Islamophobic?, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv('RedditComments.csv')\n",
    "print(len(reddit.index))\n",
    "reddit = reddit.drop_duplicates(subset='Text', keep=\"last\")\n",
    "print(len(reddit.index))\n",
    "reddit['Islamophobic?'] = reddit['Islamophobic']\n",
    "\n",
    "reddit['text'] = reddit['Text']\n",
    "reddit = reddit[['Islamophobic?', 'text']]\n",
    "df = df[['Islamophobic?', 'text']]\n",
    "result = pd.concat([df, reddit])\n",
    "\n",
    "result['Islamophobic?'] = result['Islamophobic?'].replace({'1.0': 1.0, '0.0': 0.0})\n",
    "result['Islamophobic?'] =result['Islamophobic?'].replace({'Yes': 1.0, 'No': 0.0})\n",
    "result['Islamophobic?'] =result['Islamophobic?'].replace({'1': 1.0, '0': 0.0})\n",
    "\n",
    "result.head()\n",
    "print(result['Islamophobic?'].value_counts())\n",
    "result.to_csv('prelim_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
